#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BroadTopicExtractionæ¨¡å¡Š - ä¸»ç¨‹åº
æ•´åˆè©±é¡Œæå–çš„å®Œæ•´å·¥ä½œæµç¨‹å’Œå‘½ä»¤è¡Œå·¥å…·
"""

import sys
import asyncio
import argparse
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Optional
from loguru import logger

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    from BroadTopicExtraction.get_today_news import NewsCollector, SOURCE_NAMES
    from BroadTopicExtraction.topic_extractor import TopicExtractor
    from BroadTopicExtraction.database_manager import DatabaseManager
except ImportError as e:
    logger.exception(f"å°å…¥æ¨¡å¡Šå¤±æ•—: {e}")
    logger.error("è«‹ç¢ºä¿åœ¨é …ç›®æ ¹ç›®éŒ„é‹è¡Œï¼Œä¸¦ä¸”å·²å®‰è£æ‰€æœ‰ä¾è³´")
    sys.exit(1)

class BroadTopicExtraction:
    """BroadTopicExtractionä¸»è¦å·¥ä½œæµç¨‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–"""
        self.news_collector = NewsCollector()
        self.topic_extractor = TopicExtractor()
        self.db_manager = DatabaseManager()
        
        logger.info("BroadTopicExtraction åˆå§‹åŒ–å®Œæˆ")
    
    def close(self):
        """é—œé–‰è³‡æº"""
        if self.news_collector:
            self.news_collector.close()
        if self.db_manager:
            self.db_manager.close()
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        self.close()
    
    async def run_daily_extraction(self, 
                                  news_sources: Optional[List[str]] = None,
                                  max_keywords: int = 100) -> Dict:
        """
        é‹è¡Œæ¯æ—¥è©±é¡Œæå–æµç¨‹
        
        Args:
            news_sources: æ–°èæºåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ”¯æŒçš„æº
            max_keywords: æœ€å¤§é—œéµè©æ•¸é‡
            
        Returns:
            åŒ…å«å®Œæ•´æå–çµæœçš„å­—å…¸
        """
        extraction_result_message = ""
        extraction_result_message += "\nMindSpider AIçˆ¬èŸ² - æ¯æ—¥è©±é¡Œæå–\n"
        extraction_result_message += f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        extraction_result_message += f"ç›®æ¨™æ—¥æœŸ: {date.today()}\n"
        
        if news_sources:
            extraction_result_message += f"æŒ‡å®šå¹³è‡º: {len(news_sources)} å€‹\n"
            for source in news_sources:
                source_name = SOURCE_NAMES.get(source, source)
                extraction_result_message += f"  - {source_name}\n"
        else:
            extraction_result_message += f"çˆ¬å–å¹³è‡º: å…¨éƒ¨ {len(SOURCE_NAMES)} å€‹å¹³è‡º\n"
        
        extraction_result_message += f"é—œéµè©æ•¸: æœ€å¤š {max_keywords} å€‹\n"
        
        logger.info(extraction_result_message)
        
        extraction_result = {
            'success': False,
            'extraction_date': date.today().isoformat(),
            'start_time': datetime.now().isoformat(),
            'news_collection': {},
            'topic_extraction': {},
            'database_save': {},
            'error': None
        }
        
        try:
            # æ­¥é©Ÿ1: æ”¶é›†æ–°è
            logger.info("ã€æ­¥é©Ÿ1ã€‘æ”¶é›†ç†±é»æ–°è...")
            news_result = await self.news_collector.collect_and_save_news(
                sources=news_sources
            )
            
            extraction_result['news_collection'] = {
                'success': news_result['success'],
                'total_news': news_result.get('total_news', 0),
                'successful_sources': news_result.get('successful_sources', 0),
                'total_sources': news_result.get('total_sources', 0)
            }
            
            if not news_result['success'] or not news_result['news_list']:
                raise Exception("æ–°èæ”¶é›†å¤±æ•—æˆ–æ²’æœ‰ç²å–åˆ°æ–°è")
            
            # æ­¥é©Ÿ2: æå–é—œéµè©å’Œç”Ÿæˆç¸½çµ
            logger.info("ã€æ­¥é©Ÿ2ã€‘æå–é—œéµè©å’Œç”Ÿæˆç¸½çµ...")
            keywords, summary = self.topic_extractor.extract_keywords_and_summary(
                news_result['news_list'], 
                max_keywords=max_keywords
            )
            
            extraction_result['topic_extraction'] = {
                'success': len(keywords) > 0,
                'keywords_count': len(keywords),
                'keywords': keywords,
                'summary': summary
            }
            
            if not keywords:
                logger.warning("è­¦å‘Š: æ²’æœ‰æå–åˆ°æœ‰æ•ˆé—œéµè©")
            
            # æ­¥é©Ÿ3: ä¿å­˜åˆ°æ•¸æ“šåº«
            logger.info("ã€æ­¥é©Ÿ3ã€‘ä¿å­˜åˆ†æçµæœåˆ°æ•¸æ“šåº«...")
            save_success = self.db_manager.save_daily_topics(
                keywords, summary, date.today()
            )
            
            extraction_result['database_save'] = {
                'success': save_success
            }
            
            extraction_result['success'] = True
            extraction_result['end_time'] = datetime.now().isoformat()
            
            logger.info("æ¯æ—¥è©±é¡Œæå–æµç¨‹å®Œæˆ!")
            
            return extraction_result
            
        except Exception as e:
            logger.exception(f"è©±é¡Œæå–æµç¨‹å¤±æ•—: {e}")
            extraction_result['error'] = str(e)
            extraction_result['end_time'] = datetime.now().isoformat()
            return extraction_result
    
    def print_extraction_results(self, extraction_result: Dict):
        """æ‰“å°æå–çµæœ"""
        extraction_result_message = ""
        
        # æ–°èæ”¶é›†çµæœ
        news_data = extraction_result.get('news_collection', {})
        extraction_result_message += f"\nğŸ“° æ–°èæ”¶é›†: {news_data.get('total_news', 0)} æ¢æ–°è\n"
        extraction_result_message += f"   æˆåŠŸæºæ•¸: {news_data.get('successful_sources', 0)}/{news_data.get('total_sources', 0)}\n"
        
        # è©±é¡Œæå–çµæœ
        topic_data = extraction_result.get('topic_extraction', {})
        keywords = topic_data.get('keywords', [])
        summary = topic_data.get('summary', '')
        
        extraction_result_message += f"\nğŸ”‘ æå–é—œéµè©: {len(keywords)} å€‹\n"
        if keywords:
            # æ¯è¡Œé¡¯ç¤º5å€‹é—œéµè©
            for i in range(0, len(keywords), 5):
                keyword_group = keywords[i:i+5]
                extraction_result_message += f"   {', '.join(keyword_group)}\n"
        
        extraction_result_message += f"\nğŸ“ æ–°èç¸½çµ:\n   {summary}\n"
        
        # æ•¸æ“šåº«ä¿å­˜çµæœ
        db_data = extraction_result.get('database_save', {})
        if db_data.get('success'):
            extraction_result_message += f"\nğŸ’¾ æ•¸æ“šåº«ä¿å­˜: æˆåŠŸ\n"
        else:
            extraction_result_message += f"\nğŸ’¾ æ•¸æ“šåº«ä¿å­˜: å¤±æ•—\n"
        
        logger.info(extraction_result_message)
    
    def get_keywords_for_crawling(self, extract_date: date = None) -> List[str]:
        """
        ç²å–ç”¨æ–¼çˆ¬å–çš„é—œéµè©åˆ—è¡¨
        
        Args:
            extract_date: æå–æ—¥æœŸï¼Œé»˜èªçˆ²ä»Šå¤©
            
        Returns:
            é—œéµè©åˆ—è¡¨
        """
        try:
            # å¾æ•¸æ“šåº«ç²å–è©±é¡Œåˆ†æ
            topics_data = self.db_manager.get_daily_topics(extract_date)
            
            if not topics_data:
                logger.info(f"æ²’æœ‰æ‰¾åˆ° {extract_date or date.today()} çš„è©±é¡Œæ•¸æ“š")
                return []
            
            keywords = topics_data['keywords']
            
            # ç”Ÿæˆæœç´¢é—œéµè©
            search_keywords = self.topic_extractor.get_search_keywords(keywords)
            
            logger.info(f"æº–å‚™äº† {len(search_keywords)} å€‹é—œéµè©ç”¨æ–¼çˆ¬å–")
            return search_keywords
            
        except Exception as e:
            logger.error(f"ç²å–çˆ¬å–é—œéµè©å¤±æ•—: {e}")
            return []
    
    def get_daily_analysis(self, target_date: date = None) -> Optional[Dict]:
        """ç²å–æŒ‡å®šæ—¥æœŸçš„åˆ†æçµæœ"""
        try:
            return self.db_manager.get_daily_topics(target_date)
        except Exception as e:
            logger.error(f"ç²å–æ¯æ—¥åˆ†æå¤±æ•—: {e}")
            return None
    
    def get_recent_analysis(self, days: int = 7) -> List[Dict]:
        """ç²å–æœ€è¿‘å¹¾å¤©çš„åˆ†æçµæœ"""
        try:
            return self.db_manager.get_recent_topics(days)
        except Exception as e:
            logger.error(f"ç²å–æœ€è¿‘åˆ†æå¤±æ•—: {e}")
            return []

# ==================== å‘½ä»¤è¡Œå·¥å…· ====================

async def run_extraction_command(sources=None, keywords_count=100, show_details=True):
    """é‹è¡Œè©±é¡Œæå–å‘½ä»¤"""
    
    try:
        async with BroadTopicExtraction() as extractor:
            # é‹è¡Œè©±é¡Œæå–
            result = await extractor.run_daily_extraction(
                news_sources=sources,
                max_keywords=keywords_count
            )
            
            if result['success']:
                if show_details:
                    # é¡¯ç¤ºè©³ç´°çµæœ
                    extractor.print_extraction_results(result)
                else:
                    # åªé¡¯ç¤ºç°¡è¦çµæœ
                    news_data = result.get('news_collection', {})
                    topic_data = result.get('topic_extraction', {})
                    
                    logger.info(f"âœ… è©±é¡Œæå–æˆåŠŸå®Œæˆ!")
                    logger.info(f"   æ”¶é›†æ–°è: {news_data.get('total_news', 0)} æ¢")
                    logger.info(f"   æå–é—œéµè©: {len(topic_data.get('keywords', []))} å€‹")
                    logger.info(f"   ç”Ÿæˆç¸½çµ: {len(topic_data.get('summary', ''))} å­—ç¬¦")
                
                # ç²å–çˆ¬å–é—œéµè©
                crawling_keywords = extractor.get_keywords_for_crawling()
                
                if crawling_keywords:
                    logger.info(f"\nğŸ”‘ çˆ²DeepSentimentCrawlingæº–å‚™çš„æœç´¢é—œéµè©:")
                    logger.info(f"   {', '.join(crawling_keywords)}")
                    
                    # ä¿å­˜é—œéµè©åˆ°æ–‡ä»¶
                    keywords_file = project_root / "data" / "daily_keywords.txt"
                    keywords_file.parent.mkdir(exist_ok=True)
                    
                    with open(keywords_file, 'w', encoding='utf-8') as f:
                        f.write('\n'.join(crawling_keywords))
                    
                    logger.info(f"   é—œéµè©å·²ä¿å­˜åˆ°: {keywords_file}")
                
                return True
                
            else:
                logger.error(f"âŒ è©±é¡Œæå–å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="MindSpideræ¯æ—¥è©±é¡Œæå–å·¥å…·")
    parser.add_argument("--sources", nargs="+", help="æŒ‡å®šæ–°èæºå¹³è‡º", 
                       choices=list(SOURCE_NAMES.keys()))
    parser.add_argument("--keywords", type=int, default=100, help="æœ€å¤§é—œéµè©æ•¸é‡ (é»˜èª100)")
    parser.add_argument("--quiet", action="store_true", help="ç°¡åŒ–è¼¸å‡ºæ¨¡å¼")
    parser.add_argument("--list-sources", action="store_true", help="é¡¯ç¤ºæ”¯æŒçš„æ–°èæº")
    
    args = parser.parse_args()
    
    # é¡¯ç¤ºæ”¯æŒçš„æ–°èæº
    if args.list_sources:
        logger.info("æ”¯æŒçš„æ–°èæºå¹³è‡º:")
        for source, name in SOURCE_NAMES.items():
            logger.info(f"  {source:<25} {name}")
        return
    
    # é©—è­‰åƒæ•¸
    if args.keywords < 1 or args.keywords > 200:
        logger.error("é—œéµè©æ•¸é‡æ‡‰åœ¨1-200ä¹‹é–“")
        sys.exit(1)
    
    # é‹è¡Œæå–
    try:
        success = asyncio.run(run_extraction_command(
            sources=args.sources,
            keywords_count=args.keywords,
            show_details=not args.quiet
        ))
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ¶ä¸­æ–·æ“ä½œ")
        sys.exit(1)

if __name__ == "__main__":
    main()
