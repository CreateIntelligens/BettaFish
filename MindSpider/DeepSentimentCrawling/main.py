#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSentimentCrawlingæ¨¡å¡Š - ä¸»å·¥ä½œæµç¨‹
åŸºæ–¼BroadTopicExtractionæå–çš„è©±é¡Œé€²è¡Œå…¨å¹³è‡ºé—œéµè©çˆ¬å–
"""

import sys
import argparse
from datetime import date, datetime
from pathlib import Path
from typing import List, Dict

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from keyword_manager import KeywordManager
from platform_crawler import PlatformCrawler

class DeepSentimentCrawling:
    """æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä¸»å·¥ä½œæµç¨‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ·±åº¦æƒ…æ„Ÿçˆ¬å–"""
        self.keyword_manager = KeywordManager()
        self.platform_crawler = PlatformCrawler()
        self.supported_platforms = ['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu']
    
    def run_daily_crawling(self, target_date: date = None, platforms: List[str] = None, 
                          max_keywords_per_platform: int = 50, 
                          max_notes_per_platform: int = 50,
                          login_type: str = "qrcode") -> Dict:
        """
        åŸ·è¡Œæ¯æ—¥çˆ¬å–ä»»å‹™
        
        Args:
            target_date: ç›®æ¨™æ—¥æœŸï¼Œé»˜èªçˆ²ä»Šå¤©
            platforms: è¦çˆ¬å–çš„å¹³è‡ºåˆ—è¡¨ï¼Œé»˜èªçˆ²æ‰€æœ‰æ”¯æŒçš„å¹³è‡º
            max_keywords_per_platform: æ¯å€‹å¹³è‡ºæœ€å¤§é—œéµè©æ•¸é‡
            max_notes_per_platform: æ¯å€‹å¹³è‡ºæœ€å¤§çˆ¬å–å…§å®¹æ•¸é‡
            login_type: ç™»éŒ„æ–¹å¼
        
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        if not target_date:
            target_date = date.today()
        
        if not platforms:
            platforms = self.supported_platforms
        
        print(f"ğŸš€ é–‹å§‹åŸ·è¡Œ {target_date} çš„æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä»»å‹™")
        print(f"ç›®æ¨™å¹³è‡º: {platforms}")
        
        # 1. ç²å–é—œéµè©æ‘˜è¦
        summary = self.keyword_manager.get_crawling_summary(target_date)
        print(f"ğŸ“Š é—œéµè©æ‘˜è¦: {summary}")
        
        if not summary['has_data']:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°è©±é¡Œæ•¸æ“šï¼Œç„¡æ³•é€²è¡Œçˆ¬å–")
            return {"success": False, "error": "æ²’æœ‰è©±é¡Œæ•¸æ“š"}
        
        # 2. ç²å–é—œéµè©ï¼ˆä¸åˆ†é…ï¼Œæ‰€æœ‰å¹³è‡ºä½¿ç”¨ç›¸åŒé—œéµè©ï¼‰
        print(f"\nğŸ“ ç²å–é—œéµè©...")
        keywords = self.keyword_manager.get_latest_keywords(target_date, max_keywords_per_platform)
        
        if not keywords:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°é—œéµè©ï¼Œç„¡æ³•é€²è¡Œçˆ¬å–")
            return {"success": False, "error": "æ²’æœ‰é—œéµè©"}
        
        print(f"   ç²å–åˆ° {len(keywords)} å€‹é—œéµè©")
        print(f"   å°‡åœ¨ {len(platforms)} å€‹å¹³è‡ºä¸Šçˆ¬å–æ¯å€‹é—œéµè©")
        print(f"   ç¸½çˆ¬å–ä»»å‹™: {len(keywords)} Ã— {len(platforms)} = {len(keywords) * len(platforms)}")
        
        # 3. åŸ·è¡Œå…¨å¹³è‡ºé—œéµè©çˆ¬å–
        print(f"\nğŸ”„ é–‹å§‹å…¨å¹³è‡ºé—œéµè©çˆ¬å–...")
        crawl_results = self.platform_crawler.run_multi_platform_crawl_by_keywords(
            keywords, platforms, login_type, max_notes_per_platform
        )
        
        # 4. ç”Ÿæˆæœ€çµ‚å ±å‘Š
        final_report = {
            "date": target_date.isoformat(),
            "summary": summary,
            "crawl_results": crawl_results,
            "success": crawl_results["successful_tasks"] > 0
        }
        
        print(f"\nâœ… æ·±åº¦æƒ…æ„Ÿçˆ¬å–ä»»å‹™å®Œæˆ!")
        print(f"   æ—¥æœŸ: {target_date}")
        print(f"   æˆåŠŸä»»å‹™: {crawl_results['successful_tasks']}/{crawl_results['total_tasks']}")
        print(f"   ç¸½é—œéµè©: {crawl_results['total_keywords']} å€‹")
        print(f"   ç¸½å¹³è‡º: {crawl_results['total_platforms']} å€‹")
        print(f"   ç¸½å…§å®¹: {crawl_results['total_notes']} æ¢")
        
        return final_report
    
    def run_platform_crawling(self, platform: str, target_date: date = None,
                             max_keywords: int = 50, max_notes: int = 50,
                             login_type: str = "qrcode") -> Dict:
        """
        åŸ·è¡Œå–®å€‹å¹³è‡ºçš„çˆ¬å–ä»»å‹™
        
        Args:
            platform: å¹³è‡ºåç¨±
            target_date: ç›®æ¨™æ—¥æœŸ
            max_keywords: æœ€å¤§é—œéµè©æ•¸é‡
            max_notes: æœ€å¤§çˆ¬å–å…§å®¹æ•¸é‡
            login_type: ç™»éŒ„æ–¹å¼
        
        Returns:
            çˆ¬å–çµæœ
        """
        if platform not in self.supported_platforms:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³è‡º: {platform}")
        
        if not target_date:
            target_date = date.today()
        
        print(f"ğŸ¯ é–‹å§‹åŸ·è¡Œ {platform} å¹³è‡ºçš„çˆ¬å–ä»»å‹™ ({target_date})")
        
        # ç²å–é—œéµè©
        keywords = self.keyword_manager.get_keywords_for_platform(
            platform, target_date, max_keywords
        )
        
        if not keywords:
            print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ° {platform} å¹³è‡ºçš„é—œéµè©")
            return {"success": False, "error": "æ²’æœ‰é—œéµè©"}
        
        print(f"ğŸ“ æº–å‚™çˆ¬å– {len(keywords)} å€‹é—œéµè©")
        
        # åŸ·è¡Œçˆ¬å–
        result = self.platform_crawler.run_crawler(
            platform, keywords, login_type, max_notes
        )
        
        return result
    
    def list_available_topics(self, days: int = 7):
        """åˆ—å‡ºæœ€è¿‘å¯ç”¨çš„è©±é¡Œ"""
        print(f"ğŸ“‹ æœ€è¿‘ {days} å¤©çš„è©±é¡Œæ•¸æ“š:")
        
        recent_topics = self.keyword_manager.db_manager.get_recent_topics(days)
        
        if not recent_topics:
            print("   æš«ç„¡è©±é¡Œæ•¸æ“š")
            return
        
        for topic in recent_topics:
            extract_date = topic['extract_date']
            keywords_count = len(topic.get('keywords', []))
            summary_preview = topic.get('summary', '')[:100] + "..." if len(topic.get('summary', '')) > 100 else topic.get('summary', '')
            
            print(f"   ğŸ“… {extract_date}: {keywords_count} å€‹é—œéµè©")
            print(f"      æ‘˜è¦: {summary_preview}")
            print()
    
    def show_platform_guide(self):
        """é¡¯ç¤ºå¹³è‡ºä½¿ç”¨æŒ‡å—"""
        print("ğŸ”§ å¹³è‡ºçˆ¬å–æŒ‡å—:")
        print()
        
        platform_info = {
            'xhs': 'å°ç´…æ›¸ - ç¾å¦ã€ç”Ÿæ´»ã€æ™‚å°šå…§å®¹çˆ²ä¸»',
            'dy': 'æŠ–éŸ³ - çŸ­è¦–é »ã€å¨›æ¨‚ã€ç”Ÿæ´»å…§å®¹',
            'ks': 'å¿«æ‰‹ - ç”Ÿæ´»ã€å¨›æ¨‚ã€è¾²æ‘é¡Œæå…§å®¹',
            'bili': 'Bç«™ - ç§‘æŠ€ã€å­¸ç¿’ã€éŠæˆ²ã€å‹•æ¼«å…§å®¹',
            'wb': 'å¾®åš - ç†±é»æ–°èã€æ˜æ˜Ÿã€ç¤¾æœƒè©±é¡Œ',
            'tieba': 'ç™¾åº¦è²¼å§ - èˆˆè¶£è¨è«–ã€éŠæˆ²ã€å­¸ç¿’',
            'zhihu': 'çŸ¥ä¹ - çŸ¥è­˜å•ç­”ã€æ·±åº¦è¨è«–'
        }
        
        for platform, desc in platform_info.items():
            print(f"   {platform}: {desc}")
        
        print()
        print("ğŸ’¡ ä½¿ç”¨å»ºè­°:")
        print("   1. é¦–æ¬¡ä½¿ç”¨éœ€è¦æƒç¢¼ç™»éŒ„å„å¹³è‡º")
        print("   2. å»ºè­°å…ˆæ¸¬è©¦å–®å€‹å¹³è‡ºï¼Œç¢ºèªç™»éŒ„æ­£å¸¸")
        print("   3. çˆ¬å–æ•¸é‡ä¸å®œéå¤§ï¼Œé¿å…è¢«é™åˆ¶")
        print("   4. å¯ä»¥ä½¿ç”¨ --test æ¨¡å¼é€²è¡Œå°è¦æ¨¡æ¸¬è©¦")
    
    def close(self):
        """é—œé–‰è³‡æº"""
        if self.keyword_manager:
            self.keyword_manager.close()

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="DeepSentimentCrawling - åŸºæ–¼è©±é¡Œçš„æ·±åº¦æƒ…æ„Ÿçˆ¬å–")
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument("--date", type=str, help="ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜èªçˆ²ä»Šå¤©")
    parser.add_argument("--platform", type=str, choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'], 
                       help="æŒ‡å®šå–®å€‹å¹³è‡ºé€²è¡Œçˆ¬å–")
    parser.add_argument("--platforms", type=str, nargs='+', 
                       choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'],
                       help="æŒ‡å®šå¤šå€‹å¹³è‡ºé€²è¡Œçˆ¬å–")
    
    # çˆ¬å–åƒæ•¸
    parser.add_argument("--max-keywords", type=int, default=50, 
                       help="æ¯å€‹å¹³è‡ºæœ€å¤§é—œéµè©æ•¸é‡ (é»˜èª: 50)")
    parser.add_argument("--max-notes", type=int, default=50,
                       help="æ¯å€‹å¹³è‡ºæœ€å¤§çˆ¬å–å…§å®¹æ•¸é‡ (é»˜èª: 50)")
    parser.add_argument("--login-type", type=str, choices=['qrcode', 'phone', 'cookie'], 
                       default='qrcode', help="ç™»éŒ„æ–¹å¼ (é»˜èª: qrcode)")
    
    # åŠŸèƒ½åƒæ•¸
    parser.add_argument("--list-topics", action="store_true", help="åˆ—å‡ºæœ€è¿‘çš„è©±é¡Œæ•¸æ“š")
    parser.add_argument("--days", type=int, default=7, help="æŸ¥çœ‹æœ€è¿‘å¹¾å¤©çš„è©±é¡Œ (é»˜èª: 7)")
    parser.add_argument("--guide", action="store_true", help="é¡¯ç¤ºå¹³è‡ºä½¿ç”¨æŒ‡å—")
    parser.add_argument("--test", action="store_true", help="æ¸¬è©¦æ¨¡å¼ (å°‘é‡æ•¸æ“š)")
    
    args = parser.parse_args()
    
    # è§£ææ—¥æœŸ
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            print("âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            return
    
    # å‰µå»ºçˆ¬å–å¯¦ä¾‹
    crawler = DeepSentimentCrawling()
    
    try:
        # é¡¯ç¤ºæŒ‡å—
        if args.guide:
            crawler.show_platform_guide()
            return
        
        # åˆ—å‡ºè©±é¡Œ
        if args.list_topics:
            crawler.list_available_topics(args.days)
            return
        
        # æ¸¬è©¦æ¨¡å¼èª¿æ•´åƒæ•¸
        if args.test:
            args.max_keywords = min(args.max_keywords, 10)
            args.max_notes = min(args.max_notes, 10)
            print("æ¸¬è©¦æ¨¡å¼ï¼šé™åˆ¶é—œéµè©å’Œå…§å®¹æ•¸é‡")
        
        # å–®å¹³è‡ºçˆ¬å–
        if args.platform:
            result = crawler.run_platform_crawling(
                args.platform, target_date, args.max_keywords, 
                args.max_notes, args.login_type
            )
            
            if result['success']:
                print(f"\n{args.platform} çˆ¬å–æˆåŠŸï¼")
            else:
                print(f"\n{args.platform} çˆ¬å–å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            
            return
        
        # å¤šå¹³è‡ºçˆ¬å–
        platforms = args.platforms if args.platforms else None
        result = crawler.run_daily_crawling(
            target_date, platforms, args.max_keywords, 
            args.max_notes, args.login_type
        )
        
        if result['success']:
            print(f"\nå¤šå¹³è‡ºçˆ¬å–ä»»å‹™å®Œæˆï¼")
        else:
            print(f"\nå¤šå¹³è‡ºçˆ¬å–å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    
    except KeyboardInterrupt:
        print("\nç”¨æˆ¶ä¸­æ–·æ“ä½œ")
    except Exception as e:
        print(f"\nåŸ·è¡Œå‡ºéŒ¯: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    main()
