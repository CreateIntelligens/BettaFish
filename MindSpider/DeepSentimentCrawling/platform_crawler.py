#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSentimentCrawlingæ¨¡å¡Š - å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨
è² è²¬é…ç½®å’Œèª¿ç”¨MediaCrawleré€²è¡Œå¤šå¹³è‡ºçˆ¬å–
"""

import os
import sys
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import json
from loguru import logger

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

try:
    import config
except ImportError:
    raise ImportError("ç„¡æ³•å°å…¥config.pyé…ç½®æ–‡ä»¶")

class PlatformCrawler:
    """å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨"""
        self.mediacrawler_path = Path(__file__).parent / "MediaCrawler"
        self.supported_platforms = ['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu']
        self.crawl_stats = {}
        
        # ç¢ºä¿MediaCrawlerç›®éŒ„å­˜åœ¨
        if not self.mediacrawler_path.exists():
            raise FileNotFoundError(f"MediaCrawlerç›®éŒ„ä¸å­˜åœ¨: {self.mediacrawler_path}")
        
        logger.info(f"åˆå§‹åŒ–å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨ï¼ŒMediaCrawlerè·¯å¾‘: {self.mediacrawler_path}")
    
    def configure_mediacrawler_db(self):
        """é…ç½®MediaCrawlerä½¿ç”¨æˆ‘å€‘çš„æ•¸æ“šåº«ï¼ˆMySQLæˆ–PostgreSQLï¼‰"""
        try:
            # åˆ¤æ–·æ•¸æ“šåº«é¡å‹
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            is_postgresql = db_dialect in ("postgresql", "postgres")
            
            # ä¿®æ”¹MediaCrawlerçš„æ•¸æ“šåº«é…ç½®
            db_config_path = self.mediacrawler_path / "config" / "db_config.py"
            
            # è®€å–åŸå§‹é…ç½®
            with open(db_config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # PostgreSQLé…ç½®å€¼ï¼šå¦‚æœä½¿ç”¨PostgreSQLå‰‡ä½¿ç”¨MindSpideré…ç½®ï¼Œå¦å‰‡ä½¿ç”¨é»˜èªå€¼æˆ–ç’°å¢ƒè®Šé‡
            pg_password = config.settings.DB_PASSWORD if is_postgresql else "bettafish"
            pg_user = config.settings.DB_USER if is_postgresql else "bettafish"
            pg_host = config.settings.DB_HOST if is_postgresql else "127.0.0.1"
            pg_port = config.settings.DB_PORT if is_postgresql else 5432
            pg_db_name = config.settings.DB_NAME if is_postgresql else "bettafish"
            
            # æ›¿æ›æ•¸æ“šåº«é…ç½® - ä½¿ç”¨MindSpiderçš„æ•¸æ“šåº«é…ç½®
            new_config = f'''# è²æ˜ï¼šæœ¬ä»£ç¢¼åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…æ‡‰éµå®ˆä»¥ä¸‹åŸå‰‡ï¼š  
# 1. ä¸å¾—ç”¨æ–¼ä»»ä½•å•†æ¥­ç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ™‚æ‡‰éµå®ˆç›®æ¨™å¹³è‡ºçš„ä½¿ç”¨æ¢æ¬¾å’Œrobots.txtè¦å‰‡ã€‚  
# 3. ä¸å¾—é€²è¡Œå¤§è¦æ¨¡çˆ¬å–æˆ–å°å¹³è‡ºé€ æˆé‹ç‡Ÿå¹¹æ“¾ã€‚  
# 4. æ‡‰åˆç†æ§åˆ¶è«‹æ±‚é »ç‡ï¼Œé¿å…çµ¦ç›®æ¨™å¹³è‡ºå¸¶ä¾†ä¸å¿…è¦çš„è² æ“”ã€‚   
# 5. ä¸å¾—ç”¨æ–¼ä»»ä½•éæ³•æˆ–ä¸ç•¶çš„ç”¨é€”ã€‚
#   
# è©³ç´°è¨±å¯æ¢æ¬¾è«‹åƒé–±é …ç›®æ ¹ç›®éŒ„ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç¢¼å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸå‰‡å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¢æ¬¾ã€‚  


import os

# mysql config - ä½¿ç”¨MindSpiderçš„æ•¸æ“šåº«é…ç½®
MYSQL_DB_PWD = "{config.settings.DB_PASSWORD}"
MYSQL_DB_USER = "{config.settings.DB_USER}"
MYSQL_DB_HOST = "{config.settings.DB_HOST}"
MYSQL_DB_PORT = {config.settings.DB_PORT}
MYSQL_DB_NAME = "{config.settings.DB_NAME}"

mysql_db_config = {{
    "user": MYSQL_DB_USER,
    "password": MYSQL_DB_PWD,
    "host": MYSQL_DB_HOST,
    "port": MYSQL_DB_PORT,
    "db_name": MYSQL_DB_NAME,
}}


# redis config
REDIS_DB_HOST = "127.0.0.1"  # your redis host
REDIS_DB_PWD = os.getenv("REDIS_DB_PWD", "123456")  # your redis password
REDIS_DB_PORT = os.getenv("REDIS_DB_PORT", 6379)  # your redis port
REDIS_DB_NUM = os.getenv("REDIS_DB_NUM", 0)  # your redis db num

# cache type
CACHE_TYPE_REDIS = "redis"
CACHE_TYPE_MEMORY = "memory"

# sqlite config
SQLITE_DB_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "database", "sqlite_tables.db")

sqlite_db_config = {{
    "db_path": SQLITE_DB_PATH
}}

# postgresql config - ä½¿ç”¨MindSpiderçš„æ•¸æ“šåº«é…ç½®ï¼ˆå¦‚æœDB_DIALECTæ˜¯postgresqlï¼‰æˆ–ç’°å¢ƒè®Šé‡
POSTGRESQL_DB_PWD = os.getenv("POSTGRESQL_DB_PWD", "{pg_password}")
POSTGRESQL_DB_USER = os.getenv("POSTGRESQL_DB_USER", "{pg_user}")
POSTGRESQL_DB_HOST = os.getenv("POSTGRESQL_DB_HOST", "{pg_host}")
POSTGRESQL_DB_PORT = os.getenv("POSTGRESQL_DB_PORT", "{pg_port}")
POSTGRESQL_DB_NAME = os.getenv("POSTGRESQL_DB_NAME", "{pg_db_name}")

postgresql_db_config = {{
    "user": POSTGRESQL_DB_USER,
    "password": POSTGRESQL_DB_PWD,
    "host": POSTGRESQL_DB_HOST,
    "port": POSTGRESQL_DB_PORT,
    "db_name": POSTGRESQL_DB_NAME,
}}

'''
            
            # å¯«å…¥æ–°é…ç½®
            with open(db_config_path, 'w', encoding='utf-8') as f:
                f.write(new_config)
            
            db_type = "PostgreSQL" if is_postgresql else "MySQL"
            logger.info(f"å·²é…ç½®MediaCrawlerä½¿ç”¨MindSpider {db_type}æ•¸æ“šåº«")
            return True
            
        except Exception as e:
            logger.exception(f"é…ç½®MediaCrawleræ•¸æ“šåº«å¤±æ•—: {e}")
            return False
    
    def create_base_config(self, platform: str, keywords: List[str], 
                          crawler_type: str = "search", max_notes: int = 50) -> bool:
        """
        å‰µå»ºMediaCrawlerçš„åŸºç¤é…ç½®
        
        Args:
            platform: å¹³è‡ºåç¨±
            keywords: é—œéµè©åˆ—è¡¨
            crawler_type: çˆ¬å–é¡å‹
            max_notes: æœ€å¤§çˆ¬å–æ•¸é‡
        
        Returns:
            æ˜¯å¦é…ç½®æˆåŠŸ
        """
        try:
            # åˆ¤æ–·æ•¸æ“šåº«é¡å‹ï¼Œç¢ºå®š SAVE_DATA_OPTION
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            is_postgresql = db_dialect in ("postgresql", "postgres")
            save_data_option = "postgresql" if is_postgresql else "db"
            
            base_config_path = self.mediacrawler_path / "config" / "base_config.py"
            
            # å°‡é—œéµè©åˆ—è¡¨è½‰æ›çˆ²é€—è™Ÿåˆ†éš”çš„å­—ç¬¦ä¸²
            keywords_str = ",".join(keywords)
            
            # è®€å–åŸå§‹é…ç½®æ–‡ä»¶
            with open(base_config_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ä¿®æ”¹é—œéµé…ç½®é …
            lines = content.split('\n')
            new_lines = []
            
            for line in lines:
                if line.startswith('PLATFORM = '):
                    new_lines.append(f'PLATFORM = "{platform}"  # å¹³è‡ºï¼Œxhs | dy | ks | bili | wb | tieba | zhihu')
                elif line.startswith('KEYWORDS = '):
                    new_lines.append(f'KEYWORDS = "{keywords_str}"  # é—œéµè©æœç´¢é…ç½®ï¼Œä»¥è‹±æ–‡é€—è™Ÿåˆ†éš”')
                elif line.startswith('CRAWLER_TYPE = '):
                    new_lines.append(f'CRAWLER_TYPE = "{crawler_type}"  # çˆ¬å–é¡å‹ï¼Œsearch(é—œéµè©æœç´¢) | detail(å¸–å­è©³æƒ…)| creator(å‰µä½œè€…ä¸»é æ•¸æ“š)')
                elif line.startswith('SAVE_DATA_OPTION = '):
                    new_lines.append(f'SAVE_DATA_OPTION = "{save_data_option}"  # csv or db or json or sqlite or postgresql')
                elif line.startswith('CRAWLER_MAX_NOTES_COUNT = '):
                    new_lines.append(f'CRAWLER_MAX_NOTES_COUNT = {max_notes}')
                elif line.startswith('ENABLE_GET_COMMENTS = '):
                    new_lines.append('ENABLE_GET_COMMENTS = True')
                elif line.startswith('CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = '):
                    new_lines.append('CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES = 20')
                elif line.startswith('HEADLESS = '):
                    new_lines.append('HEADLESS = True')  # ä½¿ç”¨ç„¡é ­æ¨¡å¼
                else:
                    new_lines.append(line)
            
            # å¯«å…¥æ–°é…ç½®
            with open(base_config_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(new_lines))
            
            logger.info(f"å·²é…ç½® {platform} å¹³è‡ºï¼Œçˆ¬å–é¡å‹: {crawler_type}ï¼Œé—œéµè©æ•¸é‡: {len(keywords)}ï¼Œæœ€å¤§çˆ¬å–æ•¸é‡: {max_notes}ï¼Œä¿å­˜æ•¸æ“šæ–¹å¼: {save_data_option}")
            return True
            
        except Exception as e:
            logger.exception(f"å‰µå»ºåŸºç¤é…ç½®å¤±æ•—: {e}")
            return False
    
    def run_crawler(self, platform: str, keywords: List[str], 
                   login_type: str = "qrcode", max_notes: int = 50) -> Dict:
        """
        é‹è¡Œçˆ¬èŸ²
        
        Args:
            platform: å¹³è‡ºåç¨±
            keywords: é—œéµè©åˆ—è¡¨
            login_type: ç™»éŒ„æ–¹å¼
            max_notes: æœ€å¤§çˆ¬å–æ•¸é‡
        
        Returns:
            çˆ¬å–çµæœçµ±è¨ˆ
        """
        if platform not in self.supported_platforms:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³è‡º: {platform}")
        
        if not keywords:
            raise ValueError("é—œéµè©åˆ—è¡¨ä¸èƒ½çˆ²ç©º")
        
        start_message = f"\né–‹å§‹çˆ¬å–å¹³è‡º: {platform}"
        start_message += f"\né—œéµè©: {keywords[:5]}{'...' if len(keywords) > 5 else ''} (å…±{len(keywords)}å€‹)"
        logger.info(start_message)
        
        start_time = datetime.now()
        
        try:
            # é…ç½®æ•¸æ“šåº«
            if not self.configure_mediacrawler_db():
                return {"success": False, "error": "æ•¸æ“šåº«é…ç½®å¤±æ•—"}
            
            # å‰µå»ºåŸºç¤é…ç½®
            if not self.create_base_config(platform, keywords, "search", max_notes):
                return {"success": False, "error": "åŸºç¤é…ç½®å‰µå»ºå¤±æ•—"}
            
            # åˆ¤æ–·æ•¸æ“šåº«é¡å‹ï¼Œç¢ºå®š save_data_option
            db_dialect = (config.settings.DB_DIALECT or "mysql").lower()
            is_postgresql = db_dialect in ("postgresql", "postgres")
            save_data_option = "postgresql" if is_postgresql else "db"
            
            # æ§‹å»ºå‘½ä»¤
            cmd = [
                sys.executable, "main.py",
                "--platform", platform,
                "--lt", login_type,
                "--type", "search",
                "--save_data_option", save_data_option
            ]
            
            logger.info(f"åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # åˆ‡æ›åˆ°MediaCrawlerç›®éŒ„ä¸¦åŸ·è¡Œ
            result = subprocess.run(
                cmd,
                cwd=self.mediacrawler_path,
                timeout=3600  # 60åˆ†é˜è¶…æ™‚
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # å‰µå»ºçµ±è¨ˆä¿¡æ¯
            crawl_stats = {
                "platform": platform,
                "keywords_count": len(keywords),
                "duration_seconds": duration,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "return_code": result.returncode,
                "success": result.returncode == 0,
                "notes_count": 0,
                "comments_count": 0,
                "errors_count": 0
            }
            
            # ä¿å­˜çµ±è¨ˆä¿¡æ¯
            self.crawl_stats[platform] = crawl_stats
            
            if result.returncode == 0:
                logger.info(f"âœ… {platform} çˆ¬å–å®Œæˆï¼Œè€—æ™‚: {duration:.1f}ç§’")
            else:
                logger.error(f"âŒ {platform} çˆ¬å–å¤±æ•—ï¼Œè¿”å›ç¢¼: {result.returncode}")
            
            return crawl_stats
            
        except subprocess.TimeoutExpired:
            logger.exception(f"âŒ {platform} çˆ¬å–è¶…æ™‚")
            return {"success": False, "error": "çˆ¬å–è¶…æ™‚", "platform": platform}
        except Exception as e:
            logger.exception(f"âŒ {platform} çˆ¬å–ç•°å¸¸: {e}")
            return {"success": False, "error": str(e), "platform": platform}
    
    def _parse_crawl_output(self, output_lines: List[str], error_lines: List[str]) -> Dict:
        """è§£æçˆ¬å–è¼¸å‡ºï¼Œæå–çµ±è¨ˆä¿¡æ¯"""
        stats = {
            "notes_count": 0,
            "comments_count": 0,
            "errors_count": 0,
            "login_required": False
        }
        
        # è§£æè¼¸å‡ºè¡Œ
        for line in output_lines:
            if "æ¢ç­†è¨˜" in line or "æ¢å…§å®¹" in line:
                try:
                    # æå–æ•¸å­—
                    import re
                    numbers = re.findall(r'\d+', line)
                    if numbers:
                        stats["notes_count"] = int(numbers[0])
                except:
                    pass
            elif "æ¢è©•è«–" in line:
                try:
                    import re
                    numbers = re.findall(r'\d+', line)
                    if numbers:
                        stats["comments_count"] = int(numbers[0])
                except:
                    pass
            elif "ç™»éŒ„" in line or "æƒç¢¼" in line:
                stats["login_required"] = True
        
        # è§£æéŒ¯èª¤è¡Œ
        for line in error_lines:
            if "error" in line.lower() or "ç•°å¸¸" in line:
                stats["errors_count"] += 1
        
        return stats
    
    def run_multi_platform_crawl_by_keywords(self, keywords: List[str], platforms: List[str],
                                            login_type: str = "qrcode", max_notes_per_keyword: int = 50) -> Dict:
        """
        åŸºæ–¼é—œéµè©çš„å¤šå¹³è‡ºçˆ¬å– - æ¯å€‹é—œéµè©åœ¨æ‰€æœ‰å¹³è‡ºä¸Šéƒ½é€²è¡Œçˆ¬å–
        
        Args:
            keywords: é—œéµè©åˆ—è¡¨
            platforms: å¹³è‡ºåˆ—è¡¨
            login_type: ç™»éŒ„æ–¹å¼
            max_notes_per_keyword: æ¯å€‹é—œéµè©åœ¨æ¯å€‹å¹³è‡ºçš„æœ€å¤§çˆ¬å–æ•¸é‡
        
        Returns:
            ç¸½é«”çˆ¬å–çµ±è¨ˆ
        """
        
        start_message = f"\nğŸš€ é–‹å§‹å…¨å¹³è‡ºé—œéµè©çˆ¬å–"
        start_message += f"\n   é—œéµè©æ•¸é‡: {len(keywords)}"
        start_message += f"\n   å¹³è‡ºæ•¸é‡: {len(platforms)}"
        start_message += f"\n   ç™»éŒ„æ–¹å¼: {login_type}"
        start_message += f"\n   æ¯å€‹é—œéµè©åœ¨æ¯å€‹å¹³è‡ºçš„æœ€å¤§çˆ¬å–æ•¸é‡: {max_notes_per_keyword}"
        start_message += f"\n   ç¸½çˆ¬å–ä»»å‹™: {len(keywords)} Ã— {len(platforms)} = {len(keywords) * len(platforms)}"
        logger.info(start_message)
        
        total_stats = {
            "total_keywords": len(keywords),
            "total_platforms": len(platforms),
            "total_tasks": len(keywords) * len(platforms),
            "successful_tasks": 0,
            "failed_tasks": 0,
            "total_notes": 0,
            "total_comments": 0,
            "keyword_results": {},
            "platform_summary": {}
        }
        
        # åˆå§‹åŒ–å¹³è‡ºçµ±è¨ˆ
        for platform in platforms:
            total_stats["platform_summary"][platform] = {
                "successful_keywords": 0,
                "failed_keywords": 0,
                "total_notes": 0,
                "total_comments": 0
            }
        
        # å°æ¯å€‹å¹³è‡ºä¸€æ¬¡æ€§çˆ¬å–æ‰€æœ‰é—œéµè©
        for platform in platforms:
            logger.info(f"\nğŸ“ åœ¨ {platform} å¹³è‡ºçˆ¬å–æ‰€æœ‰é—œéµè©")
            logger.info(f"   é—œéµè©: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
            
            try:
                # ä¸€æ¬¡æ€§å‚³éæ‰€æœ‰é—œéµè©çµ¦å¹³è‡º
                result = self.run_crawler(platform, keywords, login_type, max_notes_per_keyword)
                
                if result.get("success"):
                    total_stats["successful_tasks"] += len(keywords)
                    total_stats["platform_summary"][platform]["successful_keywords"] = len(keywords)
                    
                    notes_count = result.get("notes_count", 0)
                    comments_count = result.get("comments_count", 0)
                    
                    total_stats["total_notes"] += notes_count
                    total_stats["total_comments"] += comments_count
                    total_stats["platform_summary"][platform]["total_notes"] = notes_count
                    total_stats["platform_summary"][platform]["total_comments"] = comments_count
                    
                    # çˆ²æ¯å€‹é—œéµè©è¨˜éŒ„çµæœ
                    for keyword in keywords:
                        if keyword not in total_stats["keyword_results"]:
                            total_stats["keyword_results"][keyword] = {}
                        total_stats["keyword_results"][keyword][platform] = result
                    
                    logger.info(f"   âœ… æˆåŠŸ: {notes_count} æ¢å…§å®¹, {comments_count} æ¢è©•è«–")
                else:
                    total_stats["failed_tasks"] += len(keywords)
                    total_stats["platform_summary"][platform]["failed_keywords"] = len(keywords)
                    
                    # çˆ²æ¯å€‹é—œéµè©è¨˜éŒ„å¤±æ•—çµæœ
                    for keyword in keywords:
                        if keyword not in total_stats["keyword_results"]:
                            total_stats["keyword_results"][keyword] = {}
                        total_stats["keyword_results"][keyword][platform] = result
                    
                    logger.error(f"   âŒ å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            
            except Exception as e:
                total_stats["failed_tasks"] += len(keywords)
                total_stats["platform_summary"][platform]["failed_keywords"] = len(keywords)
                error_result = {"success": False, "error": str(e)}
                
                # çˆ²æ¯å€‹é—œéµè©è¨˜éŒ„ç•°å¸¸çµæœ
                for keyword in keywords:
                    if keyword not in total_stats["keyword_results"]:
                        total_stats["keyword_results"][keyword] = {}
                    total_stats["keyword_results"][keyword][platform] = error_result
                
                logger.error(f"   âŒ ç•°å¸¸: {e}")
        
        # æ‰“å°è©³ç´°çµ±è¨ˆ
        finish_message = f"\nğŸ“Š å…¨å¹³è‡ºé—œéµè©çˆ¬å–å®Œæˆ!"
        finish_message += f"\n   ç¸½ä»»å‹™: {total_stats['total_tasks']}"
        finish_message += f"\n   æˆåŠŸ: {total_stats['successful_tasks']}"
        finish_message += f"\n   å¤±æ•—: {total_stats['failed_tasks']}"
        finish_message += f"\n   æˆåŠŸç‡: {total_stats['successful_tasks']/total_stats['total_tasks']*100:.1f}%"
        finish_message += f"\n   ç¸½å…§å®¹: {total_stats['total_notes']} æ¢"
        finish_message += f"\n   ç¸½è©•è«–: {total_stats['total_comments']} æ¢"
        logger.info(finish_message)
        
        platform_summary_message = f"\nï¿½ å„å¹³è‡ºçµ±è¨ˆ:"
        for platform, stats in total_stats["platform_summary"].items():
            success_rate = stats["successful_keywords"] / len(keywords) * 100 if keywords else 0
            platform_summary_message += f"\n   {platform}: {stats['successful_keywords']}/{len(keywords)} é—œéµè©æˆåŠŸ ({success_rate:.1f}%), "
            platform_summary_message += f"{stats['total_notes']} æ¢å…§å®¹"
        logger.info(platform_summary_message)
        
        return total_stats
    
    def get_crawl_statistics(self) -> Dict:
        """ç²å–çˆ¬å–çµ±è¨ˆä¿¡æ¯"""
        return {
            "platforms_crawled": list(self.crawl_stats.keys()),
            "total_platforms": len(self.crawl_stats),
            "detailed_stats": self.crawl_stats
        }
    
    def save_crawl_log(self, log_path: str = None):
        """ä¿å­˜çˆ¬å–æ—¥èªŒ"""
        if not log_path:
            log_path = f"crawl_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(log_path, 'w', encoding='utf-8') as f:
                json.dump(self.crawl_stats, f, ensure_ascii=False, indent=2)
            logger.info(f"çˆ¬å–æ—¥èªŒå·²ä¿å­˜åˆ°: {log_path}")
        except Exception as e:
            logger.exception(f"ä¿å­˜çˆ¬å–æ—¥èªŒå¤±æ•—: {e}")

if __name__ == "__main__":
    # æ¸¬è©¦å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨
    crawler = PlatformCrawler()
    
    # æ¸¬è©¦é…ç½®
    test_keywords = ["ç§‘æŠ€", "AI", "ç·¨ç¨‹"]
    result = crawler.run_crawler("xhs", test_keywords, max_notes=5)
    
    logger.info(f"æ¸¬è©¦çµæœ: {result}")
    logger.info("å¹³è‡ºçˆ¬èŸ²ç®¡ç†å™¨æ¸¬è©¦å®Œæˆï¼")
