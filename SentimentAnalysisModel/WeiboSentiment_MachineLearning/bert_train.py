# -*- coding: utf-8 -*-
"""
BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´è…³æœ¬
"""
import argparse
import os
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score
from typing import List, Tuple
import warnings
import requests
from pathlib import Path

from base_model import BaseModel
from utils import load_corpus_bert

# å¿½ç•¥transformersçš„è­¦å‘Š
warnings.filterwarnings("ignore")
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"


class BertDataset(Dataset):
    """BERTæ•¸æ“šé›†"""
    
    def __init__(self, data: List[Tuple[str, int]]):
        self.data = [item[0] for item in data]
        self.labels = [item[1] for item in data]
    
    def __getitem__(self, index):
        return self.data[index], self.labels[index]
    
    def __len__(self):
        return len(self.labels)


class BertClassifier(nn.Module):
    """BERTåˆ†é¡å™¨ç¶²çµ¡"""
    
    def __init__(self, input_size):
        super(BertClassifier, self).__init__()
        self.fc = nn.Linear(input_size, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        out = self.fc(x)
        out = self.sigmoid(out)
        return out


class BertModel_Custom(BaseModel):
    """BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    
    def __init__(self, model_path: str = "./model/chinese_wwm_pytorch"):
        super().__init__("BERT")
        self.model_path = model_path
        self.tokenizer = None
        self.bert = None
        self.classifier = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def _download_bert_model(self):
        """è‡ªå‹•ä¸‹è¼‰BERTé è¨“ç·´æ¨¡å‹"""
        print(f"BERTæ¨¡å‹ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è¼‰ä¸­æ–‡BERTé è¨“ç·´æ¨¡å‹...")
        print("ä¸‹è¼‰ä¾†æº: bert-base-chinese (Hugging Face)")
        
        try:
            # å‰µå»ºæ¨¡å‹ç›®éŒ„
            os.makedirs(self.model_path, exist_ok=True)
            
            # ä½¿ç”¨Hugging Faceçš„ä¸­æ–‡BERTæ¨¡å‹
            model_name = "bert-base-chinese"
            print(f"æ­£åœ¨å¾Hugging Faceä¸‹è¼‰ {model_name}...")
            
            # ä¸‹è¼‰tokenizer
            print("ä¸‹è¼‰åˆ†è©å™¨...")
            tokenizer = BertTokenizer.from_pretrained(model_name)
            tokenizer.save_pretrained(self.model_path)
            
            # ä¸‹è¼‰æ¨¡å‹
            print("ä¸‹è¼‰BERTæ¨¡å‹...")
            bert_model = BertModel.from_pretrained(model_name)
            bert_model.save_pretrained(self.model_path)
            
            print(f"âœ… BERTæ¨¡å‹ä¸‹è¼‰å®Œæˆï¼Œä¿å­˜åœ¨: {self.model_path}")
            return True
            
        except Exception as e:
            print(f"âŒ BERTæ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
            print("\nğŸ’¡ æ‚¨å¯ä»¥æ‰‹å‹•ä¸‹è¼‰BERTæ¨¡å‹:")
            print("1. è¨ªå• https://huggingface.co/bert-base-chinese")
            print("2. æˆ–ä½¿ç”¨å“ˆå·¥å¤§ä¸­æ–‡BERT: https://github.com/ymcui/Chinese-BERT-wwm")
            print(f"3. å°‡æ¨¡å‹æ–‡ä»¶è§£å£“åˆ°: {self.model_path}")
            return False
    
    def _load_bert(self):
        """åŠ è¼‰BERTæ¨¡å‹å’Œåˆ†è©å™¨"""
        print(f"åŠ è¼‰BERTæ¨¡å‹: {self.model_path}")
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œå˜—è©¦è‡ªå‹•ä¸‹è¼‰
        if not os.path.exists(self.model_path) or not any(os.scandir(self.model_path)):
            print("BERTæ¨¡å‹ä¸å­˜åœ¨ï¼Œå˜—è©¦è‡ªå‹•ä¸‹è¼‰...")
            if not self._download_bert_model():
                raise FileNotFoundError(f"BERTæ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œè«‹æ‰‹å‹•ä¸‹è¼‰åˆ°: {self.model_path}")
        
        try:
            self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
            self.bert = BertModel.from_pretrained(self.model_path).to(self.device)
            
            # å‡çµBERTåƒæ•¸
            for param in self.bert.parameters():
                param.requires_grad = False
                
            print("âœ… BERTæ¨¡å‹åŠ è¼‰å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ BERTæ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
            print("å˜—è©¦ä½¿ç”¨åœ¨ç·šæ¨¡å‹...")
            
            # å¦‚æœæœ¬åœ°åŠ è¼‰å¤±æ•—ï¼Œå˜—è©¦ç›´æ¥ä½¿ç”¨åœ¨ç·šæ¨¡å‹
            try:
                model_name = "bert-base-chinese"
                self.tokenizer = BertTokenizer.from_pretrained(model_name)
                self.bert = BertModel.from_pretrained(model_name).to(self.device)
                
                # å‡çµBERTåƒæ•¸
                for param in self.bert.parameters():
                    param.requires_grad = False
                    
                print("âœ… åœ¨ç·šBERTæ¨¡å‹åŠ è¼‰å®Œæˆ")
                
            except Exception as e2:
                print(f"âŒ åœ¨ç·šæ¨¡å‹ä¹ŸåŠ è¼‰å¤±æ•—: {e2}")
                raise FileNotFoundError(f"ç„¡æ³•åŠ è¼‰BERTæ¨¡å‹ï¼Œè«‹æª¢æŸ¥ç¶²çµ¡é€£æ¥æˆ–æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹åˆ°: {self.model_path}")
    
    def train(self, train_data: List[Tuple[str, int]], **kwargs) -> None:
        """è¨“ç·´BERTæ¨¡å‹"""
        print(f"é–‹å§‹è¨“ç·´ {self.model_name} æ¨¡å‹...")
        
        # åŠ è¼‰BERT
        self._load_bert()
        
        # è¶…åƒæ•¸
        learning_rate = kwargs.get('learning_rate', 1e-3)
        num_epochs = kwargs.get('num_epochs', 10)
        batch_size = kwargs.get('batch_size', 100)
        input_size = kwargs.get('input_size', 768)
        decay_rate = kwargs.get('decay_rate', 0.9)
        
        print(f"BERTè¶…åƒæ•¸: lr={learning_rate}, epochs={num_epochs}, "
              f"batch_size={batch_size}, input_size={input_size}")
        
        # å‰µå»ºæ•¸æ“šé›†
        train_dataset = BertDataset(train_data)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # å‰µå»ºåˆ†é¡å™¨
        self.classifier = BertClassifier(input_size).to(self.device)
        
        # æå¤±å‡½æ•¸å’Œå„ªåŒ–å™¨
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(self.classifier.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)
        
        # è¨“ç·´å¾ªç’°
        self.bert.eval()  # BERTå§‹çµ‚ä¿æŒè©•ä¼°æ¨¡å¼
        self.classifier.train()
        
        for epoch in range(num_epochs):
            total_loss = 0
            num_batches = 0
            
            for i, (words, labels) in enumerate(train_loader):
                # åˆ†è©å’Œç·¨ç¢¼
                tokens = self.tokenizer(words, padding=True, truncation=True, 
                                      max_length=512, return_tensors='pt')
                input_ids = tokens["input_ids"].to(self.device)
                attention_mask = tokens["attention_mask"].to(self.device)
                labels = torch.tensor(labels, dtype=torch.float32).to(self.device)
                
                # ç²å–BERTè¼¸å‡ºï¼ˆå‡çµåƒæ•¸ï¼‰
                with torch.no_grad():
                    bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
                    bert_output = bert_outputs[0][:, 0]  # [CLS] tokençš„è¼¸å‡º
                
                # åˆ†é¡å™¨å‰å‘å‚³æ’­
                optimizer.zero_grad()
                outputs = self.classifier(bert_output)
                logits = outputs.view(-1)
                loss = criterion(logits, labels)
                
                # åå‘å‚³æ’­
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                if (i + 1) % 10 == 0:
                    avg_loss = total_loss / num_batches
                    print(f"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {avg_loss:.4f}")
                    total_loss = 0
                    num_batches = 0
            
            # å­¸ç¿’ç‡è¡°æ¸›
            scheduler.step()
            
            # ä¿å­˜æ¯å€‹epochçš„æ¨¡å‹
            if kwargs.get('save_each_epoch', False):
                epoch_model_path = f"./model/bert_epoch_{epoch+1}.pth"
                os.makedirs(os.path.dirname(epoch_model_path), exist_ok=True)
                torch.save(self.classifier.state_dict(), epoch_model_path)
                print(f"å·²ä¿å­˜æ¨¡å‹: {epoch_model_path}")
        
        self.is_trained = True
        print(f"{self.model_name} æ¨¡å‹è¨“ç·´å®Œæˆï¼")
    
    def predict(self, texts: List[str]) -> List[int]:
        """é æ¸¬æ–‡æœ¬æƒ…æ„Ÿ"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨trainæ–¹æ³•")
        
        predictions = []
        batch_size = 32
        
        self.bert.eval()
        self.classifier.eval()
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                # åˆ†è©å’Œç·¨ç¢¼
                tokens = self.tokenizer(batch_texts, padding=True, truncation=True,
                                      max_length=512, return_tensors='pt')
                input_ids = tokens["input_ids"].to(self.device)
                attention_mask = tokens["attention_mask"].to(self.device)
                
                # ç²å–BERTè¼¸å‡º
                bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
                bert_output = bert_outputs[0][:, 0]
                
                # åˆ†é¡å™¨é æ¸¬
                outputs = self.classifier(bert_output)
                outputs = outputs.view(-1)
                
                # è½‰æ›çˆ²é¡åˆ¥æ¨™ç±¤
                preds = (outputs > 0.5).cpu().numpy()
                predictions.extend(preds.astype(int).tolist())
        
        return predictions
    
    def predict_single(self, text: str) -> Tuple[int, float]:
        """é æ¸¬å–®æ¢æ–‡æœ¬çš„æƒ…æ„Ÿ"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè¨“ç·´ï¼Œè«‹å…ˆèª¿ç”¨trainæ–¹æ³•")
        
        self.bert.eval()
        self.classifier.eval()
        
        with torch.no_grad():
            # åˆ†è©å’Œç·¨ç¢¼
            tokens = self.tokenizer([text], padding=True, truncation=True,
                                  max_length=512, return_tensors='pt')
            input_ids = tokens["input_ids"].to(self.device)
            attention_mask = tokens["attention_mask"].to(self.device)
            
            # ç²å–BERTè¼¸å‡º
            bert_outputs = self.bert(input_ids, attention_mask=attention_mask)
            bert_output = bert_outputs[0][:, 0]
            
            # åˆ†é¡å™¨é æ¸¬
            output = self.classifier(bert_output)
            prob = output.item()
            
            prediction = int(prob > 0.5)
            confidence = prob if prediction == 1 else 1 - prob
        
        return prediction, confidence
    
    def save_model(self, model_path: str = None) -> None:
        """ä¿å­˜æ¨¡å‹"""
        if not self.is_trained:
            raise ValueError(f"æ¨¡å‹ {self.model_name} å°šæœªè¨“ç·´ï¼Œç„¡æ³•ä¿å­˜")
        
        if model_path is None:
            model_path = f"./model/{self.model_name.lower()}_model.pth"
        
        os.makedirs(os.path.dirname(model_path), exist_ok=True)
        
        # ä¿å­˜åˆ†é¡å™¨å’Œç›¸é—œä¿¡æ¯
        model_data = {
            'classifier_state_dict': self.classifier.state_dict(),
            'model_path': self.model_path,
            'input_size': 768,
            'device': str(self.device)
        }
        
        torch.save(model_data, model_path)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str) -> None:
        """åŠ è¼‰æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        model_data = torch.load(model_path, map_location=self.device)
        
        # è¨­ç½®BERTæ¨¡å‹è·¯å¾‘
        self.model_path = model_data['model_path']
        
        # åŠ è¼‰BERT
        self._load_bert()
        
        # é‡å»ºåˆ†é¡å™¨
        input_size = model_data['input_size']
        self.classifier = BertClassifier(input_size).to(self.device)
        
        # åŠ è¼‰åˆ†é¡å™¨æ¬Šé‡
        self.classifier.load_state_dict(model_data['classifier_state_dict'])
        
        self.is_trained = True
        print(f"å·²åŠ è¼‰æ¨¡å‹: {model_path}")
    
    @staticmethod
    def load_data(train_path: str, test_path: str) -> Tuple[List[Tuple[str, int]], List[Tuple[str, int]]]:
        """åŠ è¼‰BERTæ ¼å¼çš„æ•¸æ“š"""
        print("åŠ è¼‰è¨“ç·´æ•¸æ“š...")
        train_data = load_corpus_bert(train_path)
        print(f"è¨“ç·´æ•¸æ“šé‡: {len(train_data)}")
        
        print("åŠ è¼‰æ¸¬è©¦æ•¸æ“š...")
        test_data = load_corpus_bert(test_path)
        print(f"æ¸¬è©¦æ•¸æ“šé‡: {len(test_data)}")
        
        return train_data, test_data


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹è¨“ç·´')
    parser.add_argument('--train_path', type=str, default='./data/weibo2018/train.txt',
                        help='è¨“ç·´æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--test_path', type=str, default='./data/weibo2018/test.txt',
                        help='æ¸¬è©¦æ•¸æ“šè·¯å¾‘')
    parser.add_argument('--model_path', type=str, default='./model/bert_model.pth',
                        help='æ¨¡å‹ä¿å­˜è·¯å¾‘')
    parser.add_argument('--bert_path', type=str, default='./model/chinese_wwm_pytorch',
                        help='BERTé è¨“ç·´æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--epochs', type=int, default=10,
                        help='è¨“ç·´è¼ªæ•¸')
    parser.add_argument('--batch_size', type=int, default=100,
                        help='æ‰¹å¤§å°')
    parser.add_argument('--learning_rate', type=float, default=1e-3,
                        help='å­¸ç¿’ç‡')
    parser.add_argument('--eval_only', action='store_true',
                        help='åƒ…è©•ä¼°å·²æœ‰æ¨¡å‹ï¼Œä¸é€²è¡Œè¨“ç·´')
    
    args = parser.parse_args()
    
    # å‰µå»ºæ¨¡å‹
    model = BertModel_Custom(args.bert_path)
    
    if args.eval_only:
        # åƒ…è©•ä¼°æ¨¡å¼
        print("è©•ä¼°æ¨¡å¼ï¼šåŠ è¼‰å·²æœ‰æ¨¡å‹é€²è¡Œè©•ä¼°")
        model.load_model(args.model_path)
        
        # åŠ è¼‰æ¸¬è©¦æ•¸æ“š
        _, test_data = model.load_data(args.train_path, args.test_path)
        
        # è©•ä¼°æ¨¡å‹
        model.evaluate(test_data)
    else:
        # è¨“ç·´æ¨¡å¼
        # åŠ è¼‰æ•¸æ“š
        train_data, test_data = model.load_data(args.train_path, args.test_path)
        
        # è¨“ç·´æ¨¡å‹
        model.train(
            train_data,
            num_epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate
        )
        
        # è©•ä¼°æ¨¡å‹
        model.evaluate(test_data)
        
        # ä¿å­˜æ¨¡å‹
        model.save_model(args.model_path)
        
        # ç¤ºä¾‹é æ¸¬
        print("\nç¤ºä¾‹é æ¸¬:")
        test_texts = [
            "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ£’",
            "é€™éƒ¨é›»å½±å¤ªç„¡èŠäº†ï¼Œæµªè²»æ™‚é–“",
            "å“ˆå“ˆå“ˆï¼Œå¤ªæœ‰è¶£äº†"
        ]
        
        for text in test_texts:
            pred, conf = model.predict_single(text)
            sentiment = "æ­£é¢" if pred == 1 else "è² é¢"
            print(f"æ–‡æœ¬: {text}")
            print(f"é æ¸¬: {sentiment} (ç½®ä¿¡åº¦: {conf:.4f})")
            print()


if __name__ == "__main__":
    main()