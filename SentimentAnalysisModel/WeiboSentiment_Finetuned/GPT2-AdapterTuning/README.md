# 微博情感識別模型-GPT2-Adapter微調

## 項目說明
這是一個基於GPT2的微博情感二分類模型，採用Adapter微調技術。通過Adapter微調，只需訓練少量參數就可以讓模型適應情感分析任務，大幅降低計算資源需求和模型體積。

## 數據集
使用微博情感數據集(weibo_senti_100k)，包含約10萬條帶情感標註的微博內容，正負向評論各約5萬條。數據集標籤：
- 標籤0：負面情感
- 標籤1：正面情感

## 文件結構
```
GPT2-Adpter-tuning/
├── adapter.py              # Adapter層的實現
├── gpt2_adapter.py         # 針對GPT2模型的Adapter實現
├── train.py                # 訓練腳本
├── predict.py              # 簡化版預測腳本（交互式使用）
├── models/                 # 本地存儲的預訓練模型
│   └── gpt2-chinese/       # 中文GPT2模型及配置
├── dataset/                # 數據集目錄
│   └── weibo_senti_100k.csv  # 微博情感數據集
└── best_weibo_sentiment_model.pth  # 訓練好的最佳模型
```

## 技術特點

1. **參數高效微調**：相比全參數微調，僅訓練約3%的參數
2. **模型性能保持**：在僅訓練少量參數的情況下，保持良好的分類性能
3. **適用於資源受限環境**：模型體積小，推理速度快

## 環境依賴
- Python 3.6+
- PyTorch
- Transformers
- Pandas
- NumPy
- Scikit-learn
- Tqdm

## 使用方法

### 訓練模型
```bash
python train.py
```
訓練過程會自動：
- 下載並本地保存中文GPT2預訓練模型
- 加載微博情感數據集
- 訓練模型並保存最佳模型

### 情感分析預測
```bash
python predict.py
```
運行後將進入交互模式：
- 在控制檯輸入要分析的微博文本
- 系統會返回情感分析結果（正面/負面）和置信度
- 輸入'q'退出程序

## 模型結構
- 基礎模型：`uer/gpt2-chinese-cluecorpussmall`中文預訓練模型
- 模型本地保存路徑：`./models/gpt2-chinese/`
- 通過在每個GPT2Block後添加Adapter層進行微調
- 凍結原始GPT2參數，僅訓練分類器和Adapter層參數

## Adapter技術
Adapter是一種參數高效的微調技術，通過在Transformer層中插入小型的瓶頸層，實現用少量參數適應下游任務的目的。主要特點：

1. **參數高效**：相比全參數微調，Adapter只需訓練很小一部分參數
2. **防止遺忘**：保持原始預訓練模型的參數不變，避免災難性遺忘
3. **適應多任務**：可以爲不同任務訓練不同的Adapter，共享同一個基礎模型

在本項目中，我們在每個GPT2Block後添加了一個Adapter層，Adapter的隱藏層大小爲64，遠小於原始模型的隱藏層大小（通常爲768或1024）。

## 使用示例
```
使用設備: cuda
加載模型: best_weibo_sentiment_model.pth

============= 微博情感分析 =============
輸入微博內容進行分析 (輸入 'q' 退出):

請輸入微博內容: 這部電影真是太好看了，我非常喜歡！
預測結果: 正面情感 (置信度: 0.9876)

請輸入微博內容: 服務態度差，價格還貴，一點都不推薦
預測結果: 負面情感 (置信度: 0.9742)
```

## 注意事項
- 預測腳本使用本地模型路徑，不需要在線下載模型
- 確保`models/gpt2-chinese/`目錄包含從訓練過程中保存的模型文件
- 首次運行train.py時會自動下載並保存模型，請確保網絡連接 