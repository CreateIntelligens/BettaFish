一種Adapter-tuning的實現方式，只提供的思路，具體可以視情況稍微修改。


這裏補充一些模型層數：
GPT-2 Small：12個GPT2Block，約有1.17億個參數。
GPT-2 Medium：24個GPT2Block，約有3.48億個參數。
GPT-2 Large：36個GPT2Block，約有7.55億個參數。
GPT-2 XL (也稱爲Extra Large)：48個GPT2Block，約有15.54億個參數。

RoBERTa Base：12個RobertaLayer，總共約有1.25億個參數。
RoBERTa Large：24個RobertaLayer，總共約有3.55億個參數。
