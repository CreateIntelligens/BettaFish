# 微博情感識別模型-GPT2-LoRA微調

## 項目說明
這是一個基於GPT2的微博情感二分類模型，採用LoRA（Low-Rank Adaptation）微調技術。通過PEFT庫實現的LoRA微調，只需訓練極少量參數就可以讓模型適應情感分析任務，大幅降低計算資源需求和模型體積。

## 數據集
使用微博情感數據集(weibo_senti_100k)，包含約10萬條帶情感標註的微博內容，正負向評論各約5萬條。數據集標籤：
- 標籤0：負面情感
- 標籤1：正面情感

## 文件結構
```
GPT2-Lora/
├── train.py                  # 訓練腳本（基於PEFT庫的LoRA實現）
├── predict.py                # 預測腳本（交互式使用）
├── requirements.txt          # 依賴包列表
├── models/                   # 本地存儲的預訓練模型
│   └── gpt2-chinese/        # 中文GPT2模型及配置
├── dataset/                  # 數據集目錄
│   └── weibo_senti_100k.csv # 微博情感數據集
└── best_weibo_sentiment_lora/ # 訓練好的LoRA權重（訓練後生成）
```

## 技術特點

1. **極度參數高效**：相比全參數微調，僅訓練約0.1%-1%的參數
2. **使用PEFT庫**：基於Hugging Face官方的參數高效微調庫，穩定可靠
3. **模型性能保持**：在僅訓練極少參數的情況下，保持良好的分類性能
4. **部署友好**：LoRA權重文件小，便於模型部署和分享

## LoRA技術優勢

LoRA (Low-Rank Adaptation) 是目前最流行的參數高效微調技術：

1. **超低參數量**：通過低秩分解，將大矩陣分解爲兩個小矩陣的乘積
2. **插件式設計**：LoRA權重可以動態加載和卸載，一個基礎模型支持多個任務
3. **訓練速度快**：參數少，訓練時間短，內存佔用小
4. **無損原模型**：原始預訓練模型權重保持不變，避免災難性遺忘

## 環境依賴

安裝所需依賴：
```bash
pip install -r requirements.txt
```

主要依賴包：
- Python 3.8+
- PyTorch 1.13+
- Transformers 4.28+
- PEFT 0.4+
- Pandas, NumPy, Scikit-learn

## 使用方法

### 1. 安裝依賴
```bash
pip install -r requirements.txt
```

### 2. 訓練模型
```bash
python train.py
```

訓練過程會自動：
- 下載並本地保存中文GPT2預訓練模型
- 加載微博情感數據集
- 使用LoRA技術訓練模型
- 保存最佳LoRA權重到 `./best_weibo_sentiment_lora/`

### 3. 情感分析預測
```bash
python predict.py
```

運行後將進入交互模式：
- 在控制檯輸入要分析的微博文本
- 系統會返回情感分析結果（正面/負面）和置信度
- 輸入'q'退出程序

## 模型配置

- **基礎模型**: `uer/gpt2-chinese-cluecorpussmall` 中文預訓練模型
- **模型本地保存路徑**: `./models/gpt2-chinese/`
- **LoRA配置**:
  - rank (r): 8 - 低秩矩陣的秩
  - alpha: 32 - 縮放因子
  - target_modules: ["c_attn", "c_proj"] - 目標線性層
  - dropout: 0.1 - 防止過擬合

## 性能對比

| 方法 | 可訓練參數佔比 | 模型文件大小 | 訓練時間 | 推理速度 |
|------|----------------|--------------|----------|----------|
| 全參數微調 | 100% | ~500MB | 長 | 慢 |
| Adapter微調 | ~3% | ~50MB | 中等 | 中等 |
| **LoRA微調** | **~0.5%** | **~2MB** | **短** | **快** |

## 使用示例

```
使用設備: cuda
LoRA模型加載成功!

============= 微博情感分析 (LoRA版) =============
輸入微博內容進行分析 (輸入 'q' 退出):

請輸入微博內容: 這部電影真是太好看了，我非常喜歡！
預測結果: 正面情感 (置信度: 0.9876)

請輸入微博內容: 服務態度差，價格還貴，一點都不推薦
預測結果: 負面情感 (置信度: 0.9742)

請輸入微博內容: q
```

## 注意事項

1. **首次運行**：首次運行 `train.py` 時會自動下載預訓練模型，請確保網絡連接
2. **GPU推薦**：雖然LoRA參數少，但建議使用GPU加速訓練
3. **模型加載**：預測時需要先有訓練好的LoRA權重文件
4. **兼容性**：基於PEFT庫實現，與Hugging Face生態系統完全兼容

## 擴展功能

- **多任務支持**：可以爲不同任務訓練不同的LoRA權重，共享同一個基礎模型
- **權重合並**：可以將多個LoRA權重合並，或將LoRA權重合併到基礎模型中
- **動態切換**：支持運行時動態加載和切換不同的LoRA權重

## 技術原理

LoRA通過在原始線性層旁邊添加兩個小的矩陣A和B，使得：
```
h = W₀x + BAx
```
其中：
- W₀是凍結的預訓練權重
- B ∈ ℝᵈˣʳ, A ∈ ℝʳˣᵏ是可訓練的低秩矩陣
- r << min(d,k)，大大減少了參數量

這種設計既保持了預訓練模型的知識，又能高效地適應新任務。