# 微調Qwen3小參數模型來完成情感分析任務

<img src="https://github.com/666ghj/Weibo_PublicOpinion_AnalysisSystem/blob/main/static/image/logo_Qweb3.jpg" alt="微博情感分析示例" width="25%" />

## 項目背景

本文件夾專門用於基於阿里Qwen3系列模型的微博情感分析任務。根據最新的模型評測結果，Qwen3的小參數模型（0.6B、4B、8B）在話題識別、情感分析等相對簡單的自然語言處理任務上表現優異，超越了傳統的BERT等基礎模型。

qwen 0.6B模型加線性分類器，做特定領域的文本分類和序列標註，優於bert，也優於235B的qwen3 few shot learning。在算力有限的情況下，性價比很高...

在經過了一些相關的調研之後，我覺的將Qwen3的一些小參數模型用在本系統中是一個不錯的選擇。

雖然這個參數在LLM時代算小，但作爲個人開發者計算資源有限，微調他們還是實屬不易，在一張A100上訓練了整整四天，求求star了

## 問題探究

另外我也比較好奇一個問題：例如對於Qwen3-Embedding-0.6B跟Qwen3-0.6B這兩個模型，前者我接一個分類頭做情感二分類，後者我進行lora微調，在同樣的數據集上訓練，哪個的效果更好，各有什麼優勢？

**在絕大多數情況下，使用 Qwen3-0.6B 進行 LoRA 微調的效果會顯著優於使用 Qwen3-Embedding-0.6B 外接分類頭，但性能不如直接接分類頭的。**

因此本模塊對於所有參數的都提供**微調**與**嵌入再接分類頭**兩個版本，供大家取捨。

我們通過一個表格來清晰地展示兩者的區別和優劣勢：

| 特性 / 維度       | 方法 A: `Qwen3-Embedding-0.6B` + 分類頭                      | 方法 B: `Qwen3-0.6B` + LoRA 微調                             |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **核心思想**      | **表示學習 (Representation Learning)**                       | **指令遵循 (Instruction Following)**                         |
| **模型學習方式**  | 凍結Embedding模型，只訓練一個非常小的分類頭（如`nn.Linear`），學習從固定文本向量到情感標籤的映射。 | 凍結大部分基礎模型參數，通過訓練LoRA“適配器”來微調模型**內部的注意力機制和知識表達**，使其學會按指令生成特定答案。 |
| **性能上限**      | **較低**。模型的理解能力被`Qwen3-Embedding-0.6B`的通用語義表示所限制，無法學習你數據集中特有的、細微的情感模式。 | **更高**。模型在微調中調整了自身對語言的理解方式，以適應你的特定任務和數據分佈，能更好地捕捉諷刺、網絡用語等複雜情感。 |
| **靈活性**        | **低**。模型只能做這一件事：輸出分類標籤。無法擴展。         | **高**。模型學會的是一個“任務技能”。你可以輕鬆修改指令，讓它輸出“積極/消極/中性”，甚至“爲什麼這是積極的？”。 |
| **訓練資源開銷**  | **極低**。只需訓練一個幾KB到幾MB的分類頭，普通CPU都能完成。顯存佔用非常小。 | **較高**。雖然LoRA效率很高，但仍需在GPU上進行，需要加載整個0.6B模型和LoRA參數到顯存中進行反向傳播。 |
| **推理速度/成本** | **極快、極低**。一次前向傳播即可獲得Embedding向量，分類頭計算可忽略不計。非常適合大規模、低延遲的生產環境。 | **較慢、較高**。需要進行自迴歸生成（一個詞一個詞地蹦），即使答案很短（如“積極”），也比一次性前向傳播慢幾個數量級。 |
| **實現複雜度**    | **簡單**。遵循BERT時代的技術範式，流程成熟，代碼直觀。       | **中等**。需要構建指令模板、配置LoRA參數、使用SFTTrainer等，比前者稍複雜，但已有成熟框架支持。 |

## 使用說明

### 環境配置
```bash
# 安裝依賴
pip install -r requirements.txt

# 激活pytorch環境
conda activate 你的環境名
```

### 訓練模型

**Embedding + 分類頭方法：**
```bash
python qwen3_embedding_universal.py
# 程序會詢問選擇模型大小（0.6B/4B/8B）
```

**LoRA微調方法：**
```bash
python qwen3_lora_universal.py  
# 程序會詢問選擇模型大小（0.6B/4B/8B）
```

**命令行參數：**
```bash
# 直接指定模型
python qwen3_embedding_universal.py --model_size 0.6B
python qwen3_lora_universal.py --model_size 4B

# 自定義參數
python qwen3_embedding_universal.py --model_size 8B --epochs 10 --batch_size 16
```

### 預測使用

**交互式預測：**
```bash
python predict_universal.py
# 程序會讓你選擇具體的模型和方法
```

**命令行預測：**
```bash
# 指定模型預測
python predict_universal.py --model_type embedding --model_size 0.6B --text "今天天氣真好"

# 加載所有模型
python predict_universal.py --load_all --text "這個電影太棒了"
```

### 注意事項

1. **顯存要求**：
   - 0.6B: 最低4GB顯存
   - 4B: 最低16GB顯存  
   - 8B: 最低32GB顯存

2. **數據格式**：每行格式爲`文本內容\t標籤`，標籤爲0（負面）或1（正面）

3. **模型選擇**：初次使用建議從0.6B模型開始測試

4. **訓練時間**：LoRA微調比Embedding方法耗時更長，建議使用GPU加速