"""
é—œéµè©å„ªåŒ–ä¸­é–“ä»¶
ä½¿ç”¨Qwen AIå°‡Agentç”Ÿæˆçš„æœç´¢è©å„ªåŒ–çˆ²æ›´é©åˆè¼¿æƒ…æ•¸æ“šåº«æŸ¥è©¢çš„é—œéµè©
"""

from openai import OpenAI
import json
import sys
import os
from typing import List, Dict, Any
from dataclasses import dataclass

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘ä»¥å°å…¥config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import settings
from loguru import logger

# æ·»åŠ utilsç›®éŒ„åˆ°Pythonè·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
utils_dir = os.path.join(root_dir, 'utils')
if utils_dir not in sys.path:
    sys.path.append(utils_dir)

from retry_helper import with_graceful_retry, SEARCH_API_RETRY_CONFIG

@dataclass
class KeywordOptimizationResponse:
    """é—œéµè©å„ªåŒ–éŸ¿æ‡‰"""
    original_query: str
    optimized_keywords: List[str]
    reasoning: str
    success: bool
    error_message: str = ""

class KeywordOptimizer:
    """
    é—œéµè©å„ªåŒ–å™¨
    ä½¿ç”¨ç¡…åŸºæµå‹•çš„Qwen3æ¨¡å‹å°‡Agentç”Ÿæˆçš„æœç´¢è©å„ªåŒ–çˆ²æ›´è²¼è¿‘çœŸå¯¦è¼¿æƒ…çš„é—œéµè©
    """
    
    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None):
        """
        åˆå§‹åŒ–é—œéµè©å„ªåŒ–å™¨
        
        Args:
            api_key: ç¡…åŸºæµå‹•APIå¯†é‘°ï¼Œå¦‚æœä¸æä¾›å‰‡å¾é…ç½®æ–‡ä»¶è®€å–
            base_url: æ¥å£åŸºç¤åœ°å€ï¼Œé»˜èªä½¿ç”¨é…ç½®æ–‡ä»¶æä¾›çš„SiliconFlowåœ°å€
        """
        self.api_key = api_key or settings.KEYWORD_OPTIMIZER_API_KEY

        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°ç¡…åŸºæµå‹•APIå¯†é‘°ï¼Œè«‹åœ¨config.pyä¸­è¨­ç½®KEYWORD_OPTIMIZER_API_KEY")

        self.base_url = base_url or settings.KEYWORD_OPTIMIZER_BASE_URL

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.model = model_name or settings.KEYWORD_OPTIMIZER_MODEL_NAME
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """
        å„ªåŒ–æœç´¢é—œéµè©
        
        Args:
            original_query: Agentç”Ÿæˆçš„åŸå§‹æœç´¢æŸ¥è©¢
            context: é¡å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æ®µè½æ¨™é¡Œã€å…§å®¹æè¿°ç­‰ï¼‰
            
        Returns:
            KeywordOptimizationResponse: å„ªåŒ–å¾Œçš„é—œéµè©åˆ—è¡¨
        """
        logger.info(f"ğŸ” é—œéµè©å„ªåŒ–ä¸­é–“ä»¶: è™•ç†æŸ¥è©¢ '{original_query}'")
        
        try:
            # æ§‹å»ºå„ªåŒ–prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(original_query, context)
            
            # èª¿ç”¨Qwen API
            response = self._call_qwen_api(system_prompt, user_prompt)
            
            if response["success"]:
                # è§£æéŸ¿æ‡‰
                content = response["content"]
                try:
                    # å˜—è©¦è§£æJSONæ ¼å¼çš„éŸ¿æ‡‰
                    if content.strip().startswith('{'):
                        parsed = json.loads(content)
                        keywords = parsed.get("keywords", [])
                        reasoning = parsed.get("reasoning", "")
                    else:
                        # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦å¾æ–‡æœ¬ä¸­æå–é—œéµè©
                        keywords = self._extract_keywords_from_text(content)
                        reasoning = content
                    
                    # é©—è­‰é—œéµè©è³ªé‡
                    validated_keywords = self._validate_keywords(keywords)
                    
                    logger.info(
                        f"âœ… å„ªåŒ–æˆåŠŸ: {len(validated_keywords)}å€‹é—œéµè©" +
                        ("" if not validated_keywords else "\n" +
                         "\n".join([f"   {i}. '{k}'" for i, k in enumerate(validated_keywords, 1)]))
                    )
                        
                    
                    
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=validated_keywords,
                        reasoning=reasoning,
                        success=True
                    )
                
                except Exception as e:
                    logger.exception(f"âš ï¸ è§£æéŸ¿æ‡‰å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ: {str(e)}")
                    # å‚™ç”¨æ–¹æ¡ˆï¼šå¾åŸå§‹æŸ¥è©¢ä¸­æå–é—œéµè©
                    fallback_keywords = self._fallback_keyword_extraction(original_query)
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=fallback_keywords,
                        reasoning="APIéŸ¿æ‡‰è§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨é—œéµè©æå–",
                        success=True
                    )
            else:
                logger.error(f"âŒ APIèª¿ç”¨å¤±æ•—: {response['error']}")
                # ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
                fallback_keywords = self._fallback_keyword_extraction(original_query)
                return KeywordOptimizationResponse(
                    original_query=original_query,
                    optimized_keywords=fallback_keywords,
                    reasoning="APIèª¿ç”¨å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨é—œéµè©æå–",
                    success=True,
                    error_message=response['error']
                )
                
        except Exception as e:
            logger.error(f"âŒ é—œéµè©å„ªåŒ–å¤±æ•—: {str(e)}")
            # æœ€çµ‚å‚™ç”¨æ–¹æ¡ˆ
            fallback_keywords = self._fallback_keyword_extraction(original_query)
            return KeywordOptimizationResponse(
                original_query=original_query,
                optimized_keywords=fallback_keywords,
                reasoning="ç³»çµ±éŒ¯èª¤ï¼Œä½¿ç”¨å‚™ç”¨é—œéµè©æå–",
                success=False,
                error_message=str(e)
            )
    
    def _build_system_prompt(self) -> str:
        """æ§‹å»ºç³»çµ±prompt"""
        return """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è¼¿æƒ…æ•¸æ“šæŒ–æ˜å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡ç”¨æˆ¶æä¾›çš„æœç´¢æŸ¥è©¢å„ªåŒ–çˆ²æ›´é©åˆåœ¨ç¤¾äº¤åª’é«”è¼¿æƒ…æ•¸æ“šåº«ä¸­æŸ¥æ‰¾çš„é—œéµè©ã€‚

**æ ¸å¿ƒåŸå‰‡**ï¼š
1. **è²¼è¿‘ç¶²æ°‘èªè¨€**ï¼šä½¿ç”¨æ™®é€šç¶²å‹åœ¨ç¤¾äº¤åª’é«”ä¸Šæœƒä½¿ç”¨çš„è©å½™
2. **é¿å…å°ˆæ¥­è¡“èª**ï¼šä¸ä½¿ç”¨"è¼¿æƒ…"ã€"å‚³æ’­"ã€"å‚¾å‘"ã€"å±•æœ›"ç­‰å®˜æ–¹è©å½™
3. **ç°¡æ½”å…·é«”**ï¼šæ¯å€‹é—œéµè©è¦éå¸¸ç°¡æ½”æ˜ç­ï¼Œä¾¿æ–¼æ•¸æ“šåº«åŒ¹é…
4. **æƒ…æ„Ÿè±å¯Œ**ï¼šåŒ…å«ç¶²æ°‘å¸¸ç”¨çš„æƒ…æ„Ÿè¡¨é”è©å½™
5. **æ•¸é‡æ§åˆ¶**ï¼šæœ€å°‘æä¾›10å€‹é—œéµè©ï¼Œæœ€å¤šæä¾›20å€‹é—œéµè©
6. **é¿å…é‡è¤‡**ï¼šä¸è¦è„«é›¢åˆå§‹æŸ¥è©¢çš„ä¸»é¡Œ

**é‡è¦æé†’**ï¼šæ¯å€‹é—œéµè©éƒ½å¿…é ˆæ˜¯ä¸€å€‹ä¸å¯åˆ†å‰²çš„ç¨ç«‹è©æ¢ï¼Œåš´ç¦åœ¨è©æ¢å…§éƒ¨åŒ…å«ç©ºæ ¼ã€‚ä¾‹å¦‚ï¼Œæ‡‰ä½¿ç”¨ "é›·è»ç­çˆ­è­°" è€Œä¸æ˜¯éŒ¯èª¤çš„ "é›·è»ç­ çˆ­è­°"ã€‚

**è¼¸å‡ºæ ¼å¼**ï¼š
è«‹ä»¥JSONæ ¼å¼è¿”å›çµæœï¼š
{
    "keywords": ["é—œéµè©1", "é—œéµè©2", "é—œéµè©3"],
    "reasoning": "é¸æ“‡é€™äº›é—œéµè©çš„ç†ç”±"
}

**ç¤ºä¾‹**ï¼š
è¼¸å…¥ï¼š"æ­¦æ¼¢å¤§å­¸è¼¿æƒ…ç®¡ç† æœªä¾†å±•æœ› ç™¼å±•è¶¨å‹¢"
è¼¸å‡ºï¼š
{
    "keywords": ["æ­¦å¤§", "æ­¦æ¼¢å¤§å­¸", "å­¸æ ¡ç®¡ç†", "å¤§å­¸", "æ•™è‚²"],
    "reasoning": "é¸æ“‡'æ­¦å¤§'å’Œ'æ­¦æ¼¢å¤§å­¸'ä½œçˆ²æ ¸å¿ƒè©å½™ï¼Œé€™æ˜¯ç¶²æ°‘æœ€å¸¸ä½¿ç”¨çš„ç¨±å‘¼ï¼›'å­¸æ ¡ç®¡ç†'æ¯”'è¼¿æƒ…ç®¡ç†'æ›´è²¼è¿‘æ—¥å¸¸è¡¨é”ï¼›é¿å…ä½¿ç”¨'æœªä¾†å±•æœ›'ã€'ç™¼å±•è¶¨å‹¢'ç­‰ç¶²æ°‘å¾ˆå°‘ä½¿ç”¨çš„å°ˆæ¥­è¡“èª"
}"""

    def _build_user_prompt(self, original_query: str, context: str) -> str:
        """æ§‹å»ºç”¨æˆ¶prompt"""
        prompt = f"è«‹å°‡ä»¥ä¸‹æœç´¢æŸ¥è©¢å„ªåŒ–çˆ²é©åˆè¼¿æƒ…æ•¸æ“šåº«æŸ¥è©¢çš„é—œéµè©ï¼š\n\nåŸå§‹æŸ¥è©¢ï¼š{original_query}"
        
        if context:
            prompt += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{context}"
        
        prompt += "\n\nè«‹è¨˜ä½ï¼šè¦ä½¿ç”¨ç¶²æ°‘åœ¨ç¤¾äº¤åª’é«”ä¸ŠçœŸå¯¦ä½¿ç”¨çš„è©å½™ï¼Œé¿å…å®˜æ–¹è¡“èªå’Œå°ˆæ¥­è©å½™ã€‚"
        
        return prompt
    
    @with_graceful_retry(SEARCH_API_RETRY_CONFIG, default_return={"success": False, "error": "é—œéµè©å„ªåŒ–æœå‹™æš«æ™‚ä¸å¯ç”¨"})
    def _call_qwen_api(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """èª¿ç”¨Qwen API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
            )

            if response.choices:
                content = response.choices[0].message.content
                return {"success": True, "content": content}
            else:
                return {"success": False, "error": "APIè¿”å›æ ¼å¼ç•°å¸¸"}
        except Exception as e:
            return {"success": False, "error": f"APIèª¿ç”¨ç•°å¸¸: {str(e)}"}
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–é—œéµè©ï¼ˆç•¶JSONè§£æå¤±æ•—æ™‚ä½¿ç”¨ï¼‰"""
        # ç°¡å–®çš„é—œéµè©æå–é‚è¼¯
        lines = text.split('\n')
        keywords = []
        
        for line in lines:
            line = line.strip()
            # æŸ¥æ‰¾å¯èƒ½çš„é—œéµè©
            if 'ï¼š' in line or ':' in line:
                parts = line.split('ï¼š') if 'ï¼š' in line else line.split(':')
                if len(parts) > 1:
                    potential_keywords = parts[1].strip()
                    # å˜—è©¦åˆ†å‰²é—œéµè©
                    if 'ã€' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split('ã€')])
                    elif ',' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split(',')])
                    else:
                        keywords.append(potential_keywords)
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
        if not keywords:
            # æŸ¥æ‰¾å¼•è™Ÿä¸­çš„å…§å®¹
            import re
            quoted_content = re.findall(r'["""\'](.*?)["""\']', text)
            keywords.extend(quoted_content)
        
        # æ¸…ç†å’Œé©—è­‰é—œéµè©
        cleaned_keywords = []
        for keyword in keywords[:20]:  # æœ€å¤š20å€‹
            keyword = keyword.strip().strip('"\'""''')
            if keyword and len(keyword) <= 20:  # åˆç†é•·åº¦
                cleaned_keywords.append(keyword)
        
        return cleaned_keywords[:20]
    
    def _validate_keywords(self, keywords: List[str]) -> List[str]:
        """é©—è­‰å’Œæ¸…ç†é—œéµè©"""
        validated = []
        
        # ä¸è‰¯é—œéµè©ï¼ˆéæ–¼å°ˆæ¥­æˆ–å®˜æ–¹ï¼‰
        bad_keywords = {
            'æ…‹åº¦åˆ†æ', 'å…¬è¡†åæ‡‰', 'æƒ…ç·’å‚¾å‘',
            'æœªä¾†å±•æœ›', 'ç™¼å±•è¶¨å‹¢', 'æˆ°ç•¥è¦åŠƒ', 'æ”¿ç­–å°å‘', 'ç®¡ç†æ©Ÿåˆ¶'
        }
        
        for keyword in keywords:
            if isinstance(keyword, str):
                keyword = keyword.strip().strip('"\'""''')
                
                # åŸºæœ¬é©—è­‰
                if (keyword and 
                    len(keyword) <= 20 and 
                    len(keyword) >= 1 and
                    not any(bad_word in keyword for bad_word in bad_keywords)):
                    validated.append(keyword)
        
        return validated[:20]  # æœ€å¤šè¿”å›20å€‹é—œéµè©
    
    def _fallback_keyword_extraction(self, original_query: str) -> List[str]:
        """å‚™ç”¨é—œéµè©æå–æ–¹æ¡ˆ"""
        # ç°¡å–®çš„é—œéµè©æå–é‚è¼¯
        # ç§»é™¤å¸¸è¦‹çš„ç„¡ç”¨è©å½™
        stop_words = {'ã€'}
        
        # åˆ†å‰²æŸ¥è©¢
        import re
        # æŒ‰ç©ºæ ¼ã€æ¨™é»åˆ†å‰²
        tokens = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]+', original_query)
        
        keywords = []
        for token in tokens:
            token = token.strip()
            if token and token not in stop_words and len(token) >= 2:
                keywords.append(token)
        
        # å¦‚æœæ²’æœ‰æœ‰æ•ˆé—œéµè©ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢çš„ç¬¬ä¸€å€‹è©
        if not keywords:
            first_word = original_query.split()[0] if original_query.split() else original_query
            keywords = [first_word] if first_word else ["ç†±é–€"]
        
        return keywords[:20]

# å…¨å±€å¯¦ä¾‹
keyword_optimizer = KeywordOptimizer()
