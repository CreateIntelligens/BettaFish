"""
Deep Search Agentä¸»é¡
æ•´åˆæ‰€æœ‰æ¨¡å¡Šï¼Œå¯¦ç¾å®Œæ•´çš„æ·±åº¦æœç´¢æµç¨‹
"""

import json
import os
import re
from datetime import datetime
from typing import Optional, Dict, Any, List, Union
from loguru import logger

from .llms import LLMClient
from .nodes import (
    ReportStructureNode,
    FirstSearchNode, 
    ReflectionNode,
    FirstSummaryNode,
    ReflectionSummaryNode,
    ReportFormattingNode
)
from .state import State
from .tools import MediaCrawlerDB, DBResponse, keyword_optimizer, multilingual_sentiment_analyzer
from .utils.config import settings, Settings
from .utils import format_search_results_for_prompt


class DeepSearchAgent:
    """Deep Search Agentä¸»é¡"""
    
    def __init__(self, config: Optional[Settings] = None):
        """
        åˆå§‹åŒ–Deep Search Agent
        
        Args:
            config: å¯é¸é…ç½®å°è±¡ï¼ˆä¸å¡«å‰‡ç”¨å…¨å±€settingsï¼‰
        """
        self.config = config or settings
        
        # åˆå§‹åŒ–LLMå®¢æˆ¶ç«¯
        self.llm_client = self._initialize_llm()
        
        
        # åˆå§‹åŒ–æœç´¢å·¥å…·é›†
        self.search_agency = MediaCrawlerDB()
        
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        self.sentiment_analyzer = multilingual_sentiment_analyzer
        
        # åˆå§‹åŒ–ç¯€é»
        self._initialize_nodes()
        
        # ç‹€æ…‹
        self.state = State()
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        
        logger.info(f"Insight Agentå·²åˆå§‹åŒ–")
        logger.info(f"ä½¿ç”¨LLM: {self.llm_client.get_model_info()}")
        logger.info(f"æœç´¢å·¥å…·é›†: MediaCrawlerDB (æ”¯æŒ5ç¨®æœ¬åœ°æ•¸æ“šåº«æŸ¥è©¢å·¥å…·)")
        logger.info(f"æƒ…æ„Ÿåˆ†æ: WeiboMultilingualSentiment (æ”¯æŒ22ç¨®èªè¨€çš„æƒ…æ„Ÿåˆ†æ)")
    
    def _initialize_llm(self) -> LLMClient:
        """åˆå§‹åŒ–LLMå®¢æˆ¶ç«¯"""
        return LLMClient(
            api_key=self.config.INSIGHT_ENGINE_API_KEY,
            model_name=self.config.INSIGHT_ENGINE_MODEL_NAME,
            base_url=self.config.INSIGHT_ENGINE_BASE_URL,
        )
    
    def _initialize_nodes(self):
        """åˆå§‹åŒ–è™•ç†ç¯€é»"""
        self.first_search_node = FirstSearchNode(self.llm_client)
        self.reflection_node = ReflectionNode(self.llm_client)
        self.first_summary_node = FirstSummaryNode(self.llm_client)
        self.reflection_summary_node = ReflectionSummaryNode(self.llm_client)
        self.report_formatting_node = ReportFormattingNode(self.llm_client)
    
    def _validate_date_format(self, date_str: str) -> bool:
        """
        é©—è­‰æ—¥æœŸæ ¼å¼æ˜¯å¦çˆ²YYYY-MM-DD
        
        Args:
            date_str: æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦çˆ²æœ‰æ•ˆæ ¼å¼
        """
        if not date_str:
            return False
        
        # æª¢æŸ¥æ ¼å¼
        pattern = r'^\d{4}-\d{2}-\d{2}$'
        if not re.match(pattern, date_str):
            return False
        
        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦æœ‰æ•ˆ
        try:
            datetime.strptime(date_str, '%Y-%m-%d')
            return True
        except ValueError:
            return False
    
    def execute_search_tool(self, tool_name: str, query: str, **kwargs) -> DBResponse:
        """
        åŸ·è¡ŒæŒ‡å®šçš„æ•¸æ“šåº«æŸ¥è©¢å·¥å…·ï¼ˆé›†æˆé—œéµè©å„ªåŒ–ä¸­é–“ä»¶å’Œæƒ…æ„Ÿåˆ†æï¼‰
        
        Args:
            tool_name: å·¥å…·åç¨±ï¼Œå¯é¸å€¼ï¼š
                - "search_hot_content": æŸ¥æ‰¾ç†±é»å…§å®¹
                - "search_topic_globally": å…¨å±€è©±é¡Œæœç´¢
                - "search_topic_by_date": æŒ‰æ—¥æœŸæœç´¢è©±é¡Œ
                - "get_comments_for_topic": ç²å–è©±é¡Œè©•è«–
                - "search_topic_on_platform": å¹³è‡ºå®šå‘æœç´¢
                - "analyze_sentiment": å°æŸ¥è©¢çµæœé€²è¡Œæƒ…æ„Ÿåˆ†æ
            query: æœç´¢é—œéµè©/è©±é¡Œ
            **kwargs: é¡å¤–åƒæ•¸ï¼ˆå¦‚start_date, end_date, platform, limit, enable_sentimentç­‰ï¼‰
                     enable_sentiment: æ˜¯å¦è‡ªå‹•å°æœç´¢çµæœé€²è¡Œæƒ…æ„Ÿåˆ†æï¼ˆé»˜èªTrueï¼‰
            
        Returns:
            DBResponseå°è±¡ï¼ˆå¯èƒ½åŒ…å«æƒ…æ„Ÿåˆ†æçµæœï¼‰
        """
        logger.info(f"  â†’ åŸ·è¡Œæ•¸æ“šåº«æŸ¥è©¢å·¥å…·: {tool_name}")
        
        # å°æ–¼ç†±é»å…§å®¹æœç´¢ï¼Œä¸éœ€è¦é—œéµè©å„ªåŒ–ï¼ˆå› çˆ²ä¸éœ€è¦queryåƒæ•¸ï¼‰
        if tool_name == "search_hot_content":
            time_period = kwargs.get("time_period", "week")
            limit = kwargs.get("limit", 100)
            response = self.search_agency.search_hot_content(time_period=time_period, limit=limit)
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œæƒ…æ„Ÿåˆ†æ
            enable_sentiment = kwargs.get("enable_sentiment", True)
            if enable_sentiment and response.results and len(response.results) > 0:
                logger.info(f"  ğŸ­ é–‹å§‹å°ç†±é»å…§å®¹é€²è¡Œæƒ…æ„Ÿåˆ†æ...")
                sentiment_analysis = self._perform_sentiment_analysis(response.results)
                if sentiment_analysis:
                    # å°‡æƒ…æ„Ÿåˆ†æçµæœæ·»åŠ åˆ°éŸ¿æ‡‰çš„parametersä¸­
                    response.parameters["sentiment_analysis"] = sentiment_analysis
                    logger.info(f"  âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
            
            return response
        
        # ç¨ç«‹æƒ…æ„Ÿåˆ†æå·¥å…·
        if tool_name == "analyze_sentiment":
            texts = kwargs.get("texts", query)  # å¯ä»¥é€šétextsåƒæ•¸å‚³éï¼Œæˆ–ä½¿ç”¨query
            sentiment_result = self.analyze_sentiment_only(texts)
            
            # æ§‹å»ºDBResponseæ ¼å¼çš„éŸ¿æ‡‰
            return DBResponse(
                tool_name="analyze_sentiment",
                parameters={
                    "texts": texts if isinstance(texts, list) else [texts],
                    **kwargs
                },
                results=[],  # æƒ…æ„Ÿåˆ†æä¸è¿”å›æœç´¢çµæœ
                results_count=0,
                metadata=sentiment_result
            )
        
        # å°æ–¼éœ€è¦æœç´¢è©çš„å·¥å…·ï¼Œä½¿ç”¨é—œéµè©å„ªåŒ–ä¸­é–“ä»¶
        optimized_response = keyword_optimizer.optimize_keywords(
            original_query=query,
            context=f"ä½¿ç”¨{tool_name}å·¥å…·é€²è¡ŒæŸ¥è©¢"
        )
        
        logger.info(f"  ğŸ” åŸå§‹æŸ¥è©¢: '{query}'")
        logger.info(f"  âœ¨ å„ªåŒ–å¾Œé—œéµè©: {optimized_response.optimized_keywords}")
        
        # ä½¿ç”¨å„ªåŒ–å¾Œçš„é—œéµè©é€²è¡Œå¤šæ¬¡æŸ¥è©¢ä¸¦æ•´åˆçµæœ
        all_results = []
        total_count = 0
        
        for keyword in optimized_response.optimized_keywords:
            logger.info(f"    æŸ¥è©¢é—œéµè©: '{keyword}'")
            
            try:
                if tool_name == "search_topic_globally":
                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼Œå¿½ç•¥agentæä¾›çš„limit_per_tableåƒæ•¸
                    limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE
                    response = self.search_agency.search_topic_globally(topic=keyword, limit_per_table=limit_per_table)
                elif tool_name == "search_topic_by_date":
                    start_date = kwargs.get("start_date")
                    end_date = kwargs.get("end_date")
                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼Œå¿½ç•¥agentæä¾›çš„limit_per_tableåƒæ•¸
                    limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_BY_DATE_LIMIT_PER_TABLE
                    if not start_date or not end_date:
                        raise ValueError("search_topic_by_dateå·¥å…·éœ€è¦start_dateå’Œend_dateåƒæ•¸")
                    response = self.search_agency.search_topic_by_date(topic=keyword, start_date=start_date, end_date=end_date, limit_per_table=limit_per_table)
                elif tool_name == "get_comments_for_topic":
                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼ŒæŒ‰é—œéµè©æ•¸é‡åˆ†é…ï¼Œä½†ä¿è­‰æœ€å°å€¼
                    limit = self.config.DEFAULT_GET_COMMENTS_FOR_TOPIC_LIMIT // len(optimized_response.optimized_keywords)
                    limit = max(limit, 50)
                    response = self.search_agency.get_comments_for_topic(topic=keyword, limit=limit)
                elif tool_name == "search_topic_on_platform":
                    platform = kwargs.get("platform")
                    start_date = kwargs.get("start_date")
                    end_date = kwargs.get("end_date")
                    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼ŒæŒ‰é—œéµè©æ•¸é‡åˆ†é…ï¼Œä½†ä¿è­‰æœ€å°å€¼
                    limit = self.config.DEFAULT_SEARCH_TOPIC_ON_PLATFORM_LIMIT // len(optimized_response.optimized_keywords)
                    limit = max(limit, 30)
                    if not platform:
                        raise ValueError("search_topic_on_platformå·¥å…·éœ€è¦platformåƒæ•¸")
                    response = self.search_agency.search_topic_on_platform(platform=platform, topic=keyword, start_date=start_date, end_date=end_date, limit=limit)
                else:
                    logger.info(f"    æœªçŸ¥çš„æœç´¢å·¥å…·: {tool_name}ï¼Œä½¿ç”¨é»˜èªå…¨å±€æœç´¢")
                    response = self.search_agency.search_topic_globally(topic=keyword, limit_per_table=self.config.DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE)
                
                # æ”¶é›†çµæœ
                if response.results:
                    logger.info(f"     æ‰¾åˆ° {len(response.results)} æ¢çµæœ")
                    all_results.extend(response.results)
                    total_count += len(response.results)
                else:
                    logger.info(f"     æœªæ‰¾åˆ°çµæœ")
                    
            except Exception as e:
                logger.error(f"      æŸ¥è©¢'{keyword}'æ™‚å‡ºéŒ¯: {str(e)}")
                continue
        
        # å»é‡å’Œæ•´åˆçµæœ
        unique_results = self._deduplicate_results(all_results)
        logger.info(f"  ç¸½è¨ˆæ‰¾åˆ° {total_count} æ¢çµæœï¼Œå»é‡å¾Œ {len(unique_results)} æ¢")
        
        # æ§‹å»ºæ•´åˆå¾Œçš„éŸ¿æ‡‰
        integrated_response = DBResponse(
            tool_name=f"{tool_name}_optimized",
            parameters={
                "original_query": query,
                "optimized_keywords": optimized_response.optimized_keywords,
                "optimization_reasoning": optimized_response.reasoning,
                **kwargs
            },
            results=unique_results,
            results_count=len(unique_results)
        )
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦é€²è¡Œæƒ…æ„Ÿåˆ†æ
        enable_sentiment = kwargs.get("enable_sentiment", True)
        if enable_sentiment and unique_results and len(unique_results) > 0:
            logger.info(f"  ğŸ­ é–‹å§‹å°æœç´¢çµæœé€²è¡Œæƒ…æ„Ÿåˆ†æ...")
            sentiment_analysis = self._perform_sentiment_analysis(unique_results)
            if sentiment_analysis:
                # å°‡æƒ…æ„Ÿåˆ†æçµæœæ·»åŠ åˆ°éŸ¿æ‡‰çš„parametersä¸­
                integrated_response.parameters["sentiment_analysis"] = sentiment_analysis
                logger.info(f"  âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
        
        return integrated_response
    
    def _deduplicate_results(self, results: List) -> List:
        """
        å»é‡æœç´¢çµæœ
        """
        seen = set()
        unique_results = []
        
        for result in results:
            # ä½¿ç”¨URLæˆ–å…§å®¹ä½œçˆ²å»é‡æ¨™è­˜
            identifier = result.url if result.url else result.title_or_content[:100]
            if identifier not in seen:
                seen.add(identifier)
                unique_results.append(result)
        
        return unique_results
    
    def _perform_sentiment_analysis(self, results: List) -> Optional[Dict[str, Any]]:
        """
        å°æœç´¢çµæœåŸ·è¡Œæƒ…æ„Ÿåˆ†æ
        
        Args:
            results: æœç´¢çµæœåˆ—è¡¨
            
        Returns:
            æƒ…æ„Ÿåˆ†æçµæœå­—å…¸ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å›None
        """
        try:
            # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ä¸”æœªè¢«ç¦ç”¨ï¼‰
            if not self.sentiment_analyzer.is_initialized and not self.sentiment_analyzer.is_disabled:
                logger.info("    åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
                if not self.sentiment_analyzer.initialize():
                    logger.info("     æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œå°‡ç›´æ¥é€å‚³åŸå§‹æ–‡æœ¬")
            elif self.sentiment_analyzer.is_disabled:
                logger.info("     æƒ…æ„Ÿåˆ†æåŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥é€å‚³åŸå§‹æ–‡æœ¬")

            # å°‡æŸ¥è©¢çµæœè½‰æ›çˆ²å­—å…¸æ ¼å¼
            results_dict = []
            for result in results:
                result_dict = {
                    "content": result.title_or_content,
                    "platform": result.platform,
                    "author": result.author_nickname,
                    "url": result.url,
                    "publish_time": str(result.publish_time) if result.publish_time else None
                }
                results_dict.append(result_dict)
            
            # åŸ·è¡Œæƒ…æ„Ÿåˆ†æ
            sentiment_analysis = self.sentiment_analyzer.analyze_query_results(
                query_results=results_dict,
                text_field="content",
                min_confidence=0.5
            )
            
            return sentiment_analysis.get("sentiment_analysis")
            
        except Exception as e:
            logger.exception(f"    âŒ æƒ…æ„Ÿåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def analyze_sentiment_only(self, texts: Union[str, List[str]]) -> Dict[str, Any]:
        """
        ç¨ç«‹çš„æƒ…æ„Ÿåˆ†æå·¥å…·
        
        Args:
            texts: å–®å€‹æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            æƒ…æ„Ÿåˆ†æçµæœ
        """
        logger.info(f"  â†’ åŸ·è¡Œç¨ç«‹æƒ…æ„Ÿåˆ†æ")
        
        try:
            # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ä¸”æœªè¢«ç¦ç”¨ï¼‰
            if not self.sentiment_analyzer.is_initialized and not self.sentiment_analyzer.is_disabled:
                logger.info("    åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
                if not self.sentiment_analyzer.initialize():
                    logger.info("     æƒ…æ„Ÿåˆ†ææ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œå°‡ç›´æ¥é€å‚³åŸå§‹æ–‡æœ¬")
            elif self.sentiment_analyzer.is_disabled:
                logger.warning("     æƒ…æ„Ÿåˆ†æåŠŸèƒ½å·²ç¦ç”¨ï¼Œç›´æ¥é€å‚³åŸå§‹æ–‡æœ¬")
            
            # åŸ·è¡Œåˆ†æ
            if isinstance(texts, str):
                result = self.sentiment_analyzer.analyze_single_text(texts)
                result_dict = result.__dict__
                response = {
                    "success": result.success and result.analysis_performed,
                    "total_analyzed": 1 if result.analysis_performed and result.success else 0,
                    "results": [result_dict]
                }
                if not result.analysis_performed:
                    response["success"] = False
                    response["warning"] = result.error_message or "æƒ…æ„Ÿåˆ†æåŠŸèƒ½ä¸å¯ç”¨ï¼Œå·²ç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬"
                return response
            else:
                texts_list = list(texts)
                batch_result = self.sentiment_analyzer.analyze_batch(texts_list, show_progress=True)
                response = {
                    "success": batch_result.analysis_performed and batch_result.success_count > 0,
                    "total_analyzed": batch_result.total_processed if batch_result.analysis_performed else 0,
                    "success_count": batch_result.success_count,
                    "failed_count": batch_result.failed_count,
                    "average_confidence": batch_result.average_confidence if batch_result.analysis_performed else 0.0,
                    "results": [result.__dict__ for result in batch_result.results]
                }
                if not batch_result.analysis_performed:
                    warning = next(
                        (r.error_message for r in batch_result.results if r.error_message),
                        "æƒ…æ„Ÿåˆ†æåŠŸèƒ½ä¸å¯ç”¨ï¼Œå·²ç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬"
                    )
                    response["success"] = False
                    response["warning"] = warning
                return response
                
        except Exception as e:
            logger.exception(f"    âŒ æƒ…æ„Ÿåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "results": []
            }
    
    def research(self, query: str, save_report: bool = True) -> str:
        """
        åŸ·è¡Œæ·±åº¦ç ”ç©¶
        
        Args:
            query: ç ”ç©¶æŸ¥è©¢
            save_report: æ˜¯å¦ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶
            
        Returns:
            æœ€çµ‚å ±å‘Šå…§å®¹
        """
        logger.info(f"\n{'='*60}")
        logger.info(f"é–‹å§‹æ·±åº¦ç ”ç©¶: {query}")
        logger.info(f"{'='*60}")
        
        try:
            # Step 1: ç”Ÿæˆå ±å‘Šçµæ§‹
            self._generate_report_structure(query)
            
            # Step 2: è™•ç†æ¯å€‹æ®µè½
            self._process_paragraphs()
            
            # Step 3: ç”Ÿæˆæœ€çµ‚å ±å‘Š
            final_report = self._generate_final_report()
            
            # Step 4: ä¿å­˜å ±å‘Š
            if save_report:
                self._save_report(final_report)

            logger.info("æ·±åº¦ç ”ç©¶å®Œæˆï¼")
            
            return final_report
            
        except Exception as e:
            logger.exception(f"ç ”ç©¶éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            raise e
    
    def _generate_report_structure(self, query: str):
        """ç”Ÿæˆå ±å‘Šçµæ§‹"""
        logger.info(f"\n[æ­¥é©Ÿ 1] ç”Ÿæˆå ±å‘Šçµæ§‹...")
        
        # å‰µå»ºå ±å‘Šçµæ§‹ç¯€é»
        report_structure_node = ReportStructureNode(self.llm_client, query)
        
        # ç”Ÿæˆçµæ§‹ä¸¦æ›´æ–°ç‹€æ…‹
        self.state = report_structure_node.mutate_state(state=self.state)
        
        _message = f"å ±å‘Šçµæ§‹å·²ç”Ÿæˆï¼Œå…± {len(self.state.paragraphs)} å€‹æ®µè½:"
        for i, paragraph in enumerate(self.state.paragraphs, 1):
            _message += f"\n  {i}. {paragraph.title}"
        logger.info(_message)
    
    def _process_paragraphs(self):
        """è™•ç†æ‰€æœ‰æ®µè½"""
        total_paragraphs = len(self.state.paragraphs)
        
        for i in range(total_paragraphs):
            logger.info(f"\n[æ­¥é©Ÿ 2.{i+1}] è™•ç†æ®µè½: {self.state.paragraphs[i].title}")
            logger.info("-" * 50)
            
            # åˆå§‹æœç´¢å’Œç¸½çµ
            self._initial_search_and_summary(i)
            
            # åæ€å¾ªç’°
            self._reflection_loop(i)
            
            # æ¨™è¨˜æ®µè½å®Œæˆ
            self.state.paragraphs[i].research.mark_completed()
            
            progress = (i + 1) / total_paragraphs * 100
            logger.info(f"æ®µè½è™•ç†å®Œæˆ ({progress:.1f}%)")
    
    def _initial_search_and_summary(self, paragraph_index: int):
        """åŸ·è¡Œåˆå§‹æœç´¢å’Œç¸½çµ"""
        paragraph = self.state.paragraphs[paragraph_index]
        
        # æº–å‚™æœç´¢è¼¸å…¥
        search_input = {
            "title": paragraph.title,
            "content": paragraph.content
        }
        
        # ç”Ÿæˆæœç´¢æŸ¥è©¢å’Œå·¥å…·é¸æ“‡
        logger.info("  - ç”Ÿæˆæœç´¢æŸ¥è©¢...")
        search_output = self.first_search_node.run(search_input)
        search_query = search_output["search_query"]
        search_tool = search_output.get("search_tool", "search_topic_globally")  # é»˜èªå·¥å…·
        reasoning = search_output["reasoning"]
        
        logger.info(f"  - æœç´¢æŸ¥è©¢: {search_query}")
        logger.info(f"  - é¸æ“‡çš„å·¥å…·: {search_tool}")
        logger.info(f"  - æ¨ç†: {reasoning}")
        
        # åŸ·è¡Œæœç´¢
        logger.info("  - åŸ·è¡Œæ•¸æ“šåº«æŸ¥è©¢...")
        
        # è™•ç†ç‰¹æ®Šåƒæ•¸
        search_kwargs = {}
        
        # è™•ç†éœ€è¦æ—¥æœŸçš„å·¥å…·
        if search_tool in ["search_topic_by_date", "search_topic_on_platform"]:
            start_date = search_output.get("start_date")
            end_date = search_output.get("end_date")
            
            if start_date and end_date:
                # é©—è­‰æ—¥æœŸæ ¼å¼
                if self._validate_date_format(start_date) and self._validate_date_format(end_date):
                    search_kwargs["start_date"] = start_date
                    search_kwargs["end_date"] = end_date
                    logger.info(f"  - æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
                else:
                    logger.info(f"    æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼ˆæ‡‰çˆ²YYYY-MM-DDï¼‰ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                    logger.info(f"      æä¾›çš„æ—¥æœŸ: start_date={start_date}, end_date={end_date}")
                    search_tool = "search_topic_globally"
            elif search_tool == "search_topic_by_date":
                logger.info(f"    search_topic_by_dateå·¥å…·ç¼ºå°‘æ™‚é–“åƒæ•¸ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                search_tool = "search_topic_globally"
        
        # è™•ç†éœ€è¦å¹³è‡ºåƒæ•¸çš„å·¥å…·
        if search_tool == "search_topic_on_platform":
            platform = search_output.get("platform")
            if platform:
                search_kwargs["platform"] = platform
                logger.info(f"  - æŒ‡å®šå¹³è‡º: {platform}")
            else:
                logger.warning(f"    search_topic_on_platformå·¥å…·ç¼ºå°‘å¹³è‡ºåƒæ•¸ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                search_tool = "search_topic_globally"
        
        # è™•ç†é™åˆ¶åƒæ•¸ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼è€Œä¸æ˜¯agentæä¾›çš„åƒæ•¸
        if search_tool == "search_hot_content":
            time_period = search_output.get("time_period", "week")
            limit = self.config.DEFAULT_SEARCH_HOT_CONTENT_LIMIT
            search_kwargs["time_period"] = time_period
            search_kwargs["limit"] = limit
        elif search_tool in ["search_topic_globally", "search_topic_by_date"]:
            if search_tool == "search_topic_globally":
                limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE
            else:  # search_topic_by_date
                limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_BY_DATE_LIMIT_PER_TABLE
            search_kwargs["limit_per_table"] = limit_per_table
        elif search_tool in ["get_comments_for_topic", "search_topic_on_platform"]:
            if search_tool == "get_comments_for_topic":
                limit = self.config.DEFAULT_GET_COMMENTS_FOR_TOPIC_LIMIT
            else:  # search_topic_on_platform
                limit = self.config.DEFAULT_SEARCH_TOPIC_ON_PLATFORM_LIMIT
            search_kwargs["limit"] = limit
        
        search_response = self.execute_search_tool(search_tool, search_query, **search_kwargs)
        
        # è½‰æ›çˆ²å…¼å®¹æ ¼å¼
        search_results = []
        if search_response and search_response.results:
            # ä½¿ç”¨é…ç½®æ–‡ä»¶æ§åˆ¶å‚³éçµ¦LLMçš„çµæœæ•¸é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
            if self.config.MAX_SEARCH_RESULTS_FOR_LLM > 0:
                max_results = min(len(search_response.results), self.config.MAX_SEARCH_RESULTS_FOR_LLM)
            else:
                max_results = len(search_response.results)  # ä¸é™åˆ¶ï¼Œå‚³éæ‰€æœ‰çµæœ
            for result in search_response.results[:max_results]:
                search_results.append({
                    'title': result.title_or_content,
                    'url': result.url or "",
                    'content': result.title_or_content,
                    'score': result.hotness_score,
                    'raw_content': result.title_or_content,
                    'published_date': result.publish_time.isoformat() if result.publish_time else None,
                    'platform': result.platform,
                    'content_type': result.content_type,
                    'author': result.author_nickname,
                    'engagement': result.engagement
                })
        
        if search_results:
            _message = f"  - æ‰¾åˆ° {len(search_results)} å€‹æœç´¢çµæœ"
            for j, result in enumerate(search_results, 1):
                date_info = f" (ç™¼ä½ˆæ–¼: {result.get('published_date', 'N/A')})" if result.get('published_date') else ""
                _message += f"\n    {j}. {result['title'][:50]}...{date_info}"
            logger.info(_message)
        else:
            logger.info("  - æœªæ‰¾åˆ°æœç´¢çµæœ")
        
        # æ›´æ–°ç‹€æ…‹ä¸­çš„æœç´¢æ­·å²
        paragraph.research.add_search_results(search_query, search_results)
        
        # ç”Ÿæˆåˆå§‹ç¸½çµ
        logger.info("  - ç”Ÿæˆåˆå§‹ç¸½çµ...")
        summary_input = {
            "title": paragraph.title,
            "content": paragraph.content,
            "search_query": search_query,
            "search_results": format_search_results_for_prompt(
                search_results, self.config.MAX_CONTENT_LENGTH
            )
        }
        
        # æ›´æ–°ç‹€æ…‹
        self.state = self.first_summary_node.mutate_state(
            summary_input, self.state, paragraph_index
        )
        
        logger.info("  - åˆå§‹ç¸½çµå®Œæˆ")
    
    def _reflection_loop(self, paragraph_index: int):
        """åŸ·è¡Œåæ€å¾ªç’°"""
        paragraph = self.state.paragraphs[paragraph_index]
        
        for reflection_i in range(self.config.MAX_REFLECTIONS):
            logger.info(f"  - åæ€ {reflection_i + 1}/{self.config.MAX_REFLECTIONS}...")
            
            # æº–å‚™åæ€è¼¸å…¥
            reflection_input = {
                "title": paragraph.title,
                "content": paragraph.content,
                "paragraph_latest_state": paragraph.research.latest_summary
            }
            
            # ç”Ÿæˆåæ€æœç´¢æŸ¥è©¢
            reflection_output = self.reflection_node.run(reflection_input)
            search_query = reflection_output["search_query"]
            search_tool = reflection_output.get("search_tool", "search_topic_globally")  # é»˜èªå·¥å…·
            reasoning = reflection_output["reasoning"]
            
            logger.info(f"    åæ€æŸ¥è©¢: {search_query}")
            logger.info(f"    é¸æ“‡çš„å·¥å…·: {search_tool}")
            logger.info(f"    åæ€æ¨ç†: {reasoning}")
            
            # åŸ·è¡Œåæ€æœç´¢
            # è™•ç†ç‰¹æ®Šåƒæ•¸
            search_kwargs = {}
            
            # è™•ç†éœ€è¦æ—¥æœŸçš„å·¥å…·
            if search_tool in ["search_topic_by_date", "search_topic_on_platform"]:
                start_date = reflection_output.get("start_date")
                end_date = reflection_output.get("end_date")
                
                if start_date and end_date:
                    # é©—è­‰æ—¥æœŸæ ¼å¼
                    if self._validate_date_format(start_date) and self._validate_date_format(end_date):
                        search_kwargs["start_date"] = start_date
                        search_kwargs["end_date"] = end_date
                        logger.info(f"    æ™‚é–“ç¯„åœ: {start_date} åˆ° {end_date}")
                    else:
                        logger.info(f"      æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼ˆæ‡‰çˆ²YYYY-MM-DDï¼‰ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                        logger.info(f"        æä¾›çš„æ—¥æœŸ: start_date={start_date}, end_date={end_date}")
                        search_tool = "search_topic_globally"
                elif search_tool == "search_topic_by_date":
                    logger.warning(f"      search_topic_by_dateå·¥å…·ç¼ºå°‘æ™‚é–“åƒæ•¸ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                    search_tool = "search_topic_globally"
            
            # è™•ç†éœ€è¦å¹³è‡ºåƒæ•¸çš„å·¥å…·
            if search_tool == "search_topic_on_platform":
                platform = reflection_output.get("platform")
                if platform:
                    search_kwargs["platform"] = platform
                    logger.info(f"    æŒ‡å®šå¹³è‡º: {platform}")
                else:
                    logger.warning(f"      search_topic_on_platformå·¥å…·ç¼ºå°‘å¹³è‡ºåƒæ•¸ï¼Œæ”¹ç”¨å…¨å±€æœç´¢")
                    search_tool = "search_topic_globally"
            
            # è™•ç†é™åˆ¶åƒæ•¸
            if search_tool == "search_hot_content":
                time_period = reflection_output.get("time_period", "week")
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼Œä¸å…è¨±agentæ§åˆ¶limitåƒæ•¸
                limit = self.config.DEFAULT_SEARCH_HOT_CONTENT_LIMIT
                search_kwargs["time_period"] = time_period
                search_kwargs["limit"] = limit
            elif search_tool in ["search_topic_globally", "search_topic_by_date"]:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼Œä¸å…è¨±agentæ§åˆ¶limit_per_tableåƒæ•¸
                if search_tool == "search_topic_globally":
                    limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_GLOBALLY_LIMIT_PER_TABLE
                else:  # search_topic_by_date
                    limit_per_table = self.config.DEFAULT_SEARCH_TOPIC_BY_DATE_LIMIT_PER_TABLE
                search_kwargs["limit_per_table"] = limit_per_table
            elif search_tool in ["get_comments_for_topic", "search_topic_on_platform"]:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜èªå€¼ï¼Œä¸å…è¨±agentæ§åˆ¶limitåƒæ•¸
                if search_tool == "get_comments_for_topic":
                    limit = self.config.DEFAULT_GET_COMMENTS_FOR_TOPIC_LIMIT
                else:  # search_topic_on_platform
                    limit = self.config.DEFAULT_SEARCH_TOPIC_ON_PLATFORM_LIMIT
                search_kwargs["limit"] = limit
            
            search_response = self.execute_search_tool(search_tool, search_query, **search_kwargs)
            
            # è½‰æ›çˆ²å…¼å®¹æ ¼å¼
            search_results = []
            if search_response and search_response.results:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶æ§åˆ¶å‚³éçµ¦LLMçš„çµæœæ•¸é‡ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
                if self.config.MAX_SEARCH_RESULTS_FOR_LLM > 0:
                    max_results = min(len(search_response.results), self.config.MAX_SEARCH_RESULTS_FOR_LLM)
                else:
                    max_results = len(search_response.results)  # ä¸é™åˆ¶ï¼Œå‚³éæ‰€æœ‰çµæœ
                for result in search_response.results[:max_results]:
                    search_results.append({
                        'title': result.title_or_content,
                        'url': result.url or "",
                        'content': result.title_or_content,
                        'score': result.hotness_score,
                        'raw_content': result.title_or_content,
                        'published_date': result.publish_time.isoformat() if result.publish_time else None,
                        'platform': result.platform,
                        'content_type': result.content_type,
                        'author': result.author_nickname,
                        'engagement': result.engagement
                    })
            
            if search_results:
                _message = f"    æ‰¾åˆ° {len(search_results)} å€‹åæ€æœç´¢çµæœ"
                for j, result in enumerate(search_results, 1):
                    date_info = f" (ç™¼ä½ˆæ–¼: {result.get('published_date', 'N/A')})" if result.get('published_date') else ""
                    _message += f"\n      {j}. {result['title'][:50]}...{date_info}"
                logger.info(_message)
            else:
                logger.info("    æœªæ‰¾åˆ°åæ€æœç´¢çµæœ")
            
            # æ›´æ–°æœç´¢æ­·å²
            paragraph.research.add_search_results(search_query, search_results)
            
            # ç”Ÿæˆåæ€ç¸½çµ
            reflection_summary_input = {
                "title": paragraph.title,
                "content": paragraph.content,
                "search_query": search_query,
                "search_results": format_search_results_for_prompt(
                    search_results, self.config.MAX_CONTENT_LENGTH
                ),
                "paragraph_latest_state": paragraph.research.latest_summary
            }
            
            # æ›´æ–°ç‹€æ…‹
            self.state = self.reflection_summary_node.mutate_state(
                reflection_summary_input, self.state, paragraph_index
            )
            
            logger.info(f"    åæ€ {reflection_i + 1} å®Œæˆ")
    
    def _generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        logger.info(f"\n[æ­¥é©Ÿ 3] ç”Ÿæˆæœ€çµ‚å ±å‘Š...")
        
        # æº–å‚™å ±å‘Šæ•¸æ“š
        report_data = []
        for paragraph in self.state.paragraphs:
            report_data.append({
                "title": paragraph.title,
                "paragraph_latest_state": paragraph.research.latest_summary
            })
        
        # æ ¼å¼åŒ–å ±å‘Š
        try:
            final_report = self.report_formatting_node.run(report_data)
        except Exception as e:
            logger.exception(f"LLMæ ¼å¼åŒ–å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•: {str(e)}")
            final_report = self.report_formatting_node.format_report_manually(
                report_data, self.state.report_title
            )
        
        # æ›´æ–°ç‹€æ…‹
        self.state.final_report = final_report
        self.state.mark_completed()
        
        logger.info("æœ€çµ‚å ±å‘Šç”Ÿæˆå®Œæˆ")
        return final_report
    
    def _save_report(self, report_content: str):
        """ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶"""
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        query_safe = "".join(c for c in self.state.query if c.isalnum() or c in (' ', '-', '_')).rstrip()
        query_safe = query_safe.replace(' ', '_')[:30]
        
        filename = f"deep_search_report_{query_safe}_{timestamp}.md"
        filepath = os.path.join(self.config.OUTPUT_DIR, filename)
        
        # ä¿å­˜å ±å‘Š
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"å ±å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        
        # ä¿å­˜ç‹€æ…‹ï¼ˆå¦‚æœé…ç½®å…è¨±ï¼‰
        if self.config.SAVE_INTERMEDIATE_STATES:
            state_filename = f"state_{query_safe}_{timestamp}.json"
            state_filepath = os.path.join(self.config.OUTPUT_DIR, state_filename)
            self.state.save_to_file(state_filepath)
            logger.info(f"ç‹€æ…‹å·²ä¿å­˜åˆ°: {state_filepath}")
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """ç²å–é€²åº¦æ‘˜è¦"""
        return self.state.get_progress_summary()
    
    def load_state(self, filepath: str):
        """å¾æ–‡ä»¶åŠ è¼‰ç‹€æ…‹"""
        self.state = State.load_from_file(filepath)
        logger.info(f"ç‹€æ…‹å·²å¾ {filepath} åŠ è¼‰")
    
    def save_state(self, filepath: str):
        """ä¿å­˜ç‹€æ…‹åˆ°æ–‡ä»¶"""
        self.state.save_to_file(filepath)
        logger.info(f"ç‹€æ…‹å·²ä¿å­˜åˆ° {filepath}")


def create_agent(config_file: Optional[str] = None) -> DeepSearchAgent:
    """
    å‰µå»ºDeep Search Agentå¯¦ä¾‹çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾‘
        
    Returns:
        DeepSearchAgentå¯¦ä¾‹
    """
    config = settings
    return DeepSearchAgent(config)
